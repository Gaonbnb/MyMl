# 基本树的部分
## ID3
总准则是奥卡姆剃刀的思想

### 思想
信息论中显示，信息熵越大，样本纯度越低。ID3算法的核心就是用信息增益度来选择特征。选择信息增益最大的特征进行分裂。算法采用自顶向下的贪婪搜索遍历可能的决策树空间。其大噶步骤为：以上参考统计学习方法的内容

算法缺点为没有剪枝策略因此容易过拟合。信息增益则可对取值数目较多的特征有所偏好，类似“编号”的特征其信息增益接近于1。只能处理离散分布的特征。没有考虑缺失值。

## ID4.5
引入信息增益率来作为分类标准
### 思想
引入悲观剪枝策略进行后剪枝。引入信息增益率作为划分标准。将连续特征离散化。对于缺失值分为两个子问题，一个是在特征值缺失的情况下进行划分特征的选择（即如何计算特征的信息增益率），另一个是选定该划分特征，对于缺失该特征值的样本如何选择（即到底把这个样本划分到哪个结点里）。针对问题一，对于具有缺失值特征，用没有缺失的样本子集所占比重来折算。针对问题二，将样本同时划分到所有子结点，不多要调整样本的权重值，其实也就是以不同的概率划分到不同的子节点。
### 划分标准
信息增益率 参考统计学习方法，中间加上了一些笔记
### 剪枝策略
首先进行预剪枝，在节点划分前来确定是否继续增长，及早停止增长的主要方法:节点内数据样本低于某一阈值；所有节点特征都已经分裂。节点划分前准确率比划分后准确率高。预剪枝是贪心算法，会带来欠拟合。

其次进行后剪枝策略，c4.5采用的是悲观剪枝方法，递归从下到上评估每一个非叶子节点，评估一个最佳叶子节点去代替这颗子树是否有益。后剪枝策略泛化能力好，但是训练时间比较长。

缺点
1. c4.5用的树是多叉树，用二叉树效率跟高
2. c4.5只能用于分类
3. 运算的时间复杂度过高
4. c4.5构造树的过程，需要对数值属性值进行排序，所以只适合能够驻留于内存的数据集，当训练集大得无法在内存容纳时，程序无法运行
 
## CART
这个算法的二分法可以简化决策树的规模，提高生成决策树的效率
### 思想
基本过程包括分裂剪枝和树选择
**分裂**：分裂过程是一个二叉递归的过程，输入和预测特征既可以是连续型与可以是离散型的，没有停止准则可以一直生长下去
**剪枝**：采用代价复杂度剪枝，从最大树开始，每次选择训练数据熵对整体性能贡献最小的那个分裂节点作为下一个剪枝对象，直到只剩下根节点。CART会产生一系列嵌套的剪枝树，需要从中选出一颗最优的决策树。
**树选择**：用单独的测试集评估每颗剪枝树的预测性能（也可以用交叉验证）

CART在c4.5上的提升：
首先只有二叉树，运算速度快。c4.5只能够分类，而CART可以分类可以回归。使用基尼系数作为变量的不纯度量，减少大量的对数运算。CART采用代理测试来估计缺失值，c4.5以不同概率划分到不同节点中。CART采用基于代价复杂度剪枝的方法进行剪枝，而c4.5采用悲观剪枝方法
### 划分标准
基尼系数反映了从数据集中随机抽取两个样本，其类别标记不一致的概率
### 缺失值处理
对c4.5的两个问题，CART一开始严格要求分裂特征评估时只能使用在该特征上没有缺失值的那部分数据。后续版本会使用一种惩罚机制来抑制提升值，从而反映出缺失值的影响。（如一个特征在节点的20%记录是缺失的，那么这个特征就会减少20%或者其他数值）
对问题二，是为树的每个节点找到代理分裂器，无论在训练数据上得到的树是否有缺失值都会这样做。在代理分裂器中，特征的分值必须超过默认规则的性能才有资格作为代理（即代理就是代替缺失值特征作为划分特征的特征），当树遇到缺失值时这个实例划分到左边还是右边是取决于其排名最高的代理，如果这个代理的值也缺失了，那就使用排名第二的代理，以此类推，若全部代理值缺失，默认规则是把样本划分到较大的那个子节点。代理分裂器可以确保无缺失训练数据上得到的树可以用来处理包含缺失值的新数据。
### 剪枝策略
参考李航统计学习方法
### 类别不平衡
训练数据在不平衡都有方法处理
使用先验机制，相当于对类别进行加权。这种先验机制嵌入于Cart算法判断分裂优劣的运算中，在cart默认的分类模式中，总是要计算每个节点关于根节点的类别频率的比值，这就相当于对数据自动重新加权，对类别进行均衡。
对于一个二分类问题，节点node被分成类别1当且仅当：
N1(node) / N1(root) > N0(node) / N0(root)
比如二分类，根节点属于1类和0类的分别有20和80个。在子节点上有30个样本，其中属于1类和0类的分别是10个和20个。如果10/20 > 20/80,该节点属于1类
### 回归树看李航统计学习方法

# 决策树——随机森林、adaboost、GBDT
## 集成学习
### bagging
booststrap aggregating
每个基学习器都会对训练集进行有放回抽样得到子训练集，每个基学习器基于不同子训练集进行训练，并综合所有基训练器的预测值进行最终的预测结果。bagging最多使用投票法，票数最多的类别为预测类别
### boosting
阶梯状训练，每个基模型都在前一个基模型的基础上训练，最终综合所有的基模型预测值产生最终结果，用的比较多的是加权法
### stacking
先用全部数据训练好基模型，然后基模型对每个训练样本进行训练得到模型，其预测值将作为训练样本的特征值，最终会得到新的训练样本，然后会基于新的训练得到模型，然后得到最终预测结果。
## 偏差与方差
可以查询经典四格图片

我们常说集成学习中的基模型是弱分类器，通常来说弱模型是偏差高（在训练集上准确性低），方差小（防止过拟合能力强）的模型，**但并不是所有集成学习框架中的基模型都是弱模型。bagging和stacking中的基模型为强模型（偏差低方差高），而boosting中的基模型为弱模型（偏差高方差低）**
## 随机森林
随机森林利用随机取样和随机选取特征训练每一棵树，基于bagging的方法。用bootstrap自主采样法。
有四个步骤：随机选择样本（有放回抽样），随机选取特征，构建决策树，进行森林投票

好处：由于引入了两个随机抽样的方法，所以每棵树都是尽可能的生长，不剪枝也不会最终过拟合。因此易于并行化的运算，能高效的处理高维度数据，不用做特征选择
## adaboost
具体模型看书，是弱分类器。采用前向分布算法。提高错误分类样本的权重，并不断迭代。
优点是分类精度高，可以用多种可分类模型构建弱分类器， 不容易过拟合
缺点是对异常值过于敏感
## GBDT
思想和李航统计学习方法中的提升树有些类似，用残差进行不断的迭代计算，用梯度的方向代替残差
更多的利用回归树

损失函数更多的是用绝对函数和huber函数防止平均残差平方和的过拟合。
利用缩减思想，认为一步走的小比一步走的大更容易避免过拟合

## adaboost和GBDT和异同点

同：都是boosting家族成员，都使用弱分类器和前向分布算法
不同：adaboost利用提升错分样本的权重来弥补数据模型，GBDT利用梯度（残差）
adaboost利用指数损失，GBDT用绝对损失或huber损失函数

