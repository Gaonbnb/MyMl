{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
    "import pandas as pd\n",
    "pd.DataFrame(data)\n",
    "\n",
    "# 实现归一化\n",
    "scaler = MinMaxScaler()\n",
    "scaler = scaler.fit(data)# 本质是生成min(x)和max(x)\n",
    "result = scaler.transform(data)#导出结果\n",
    "result# 对每一列求的最大最小\n",
    "# 或者\n",
    "result_ = scaler.fit_transform(data)\n",
    "result_\n",
    "# 将归一化的结果逆转回去\n",
    "scaler.inverse_transform(result)\n",
    "# 使用MinMaxScaler的参数feature_range实现将数据归一化到0 1 范围以外的范围中\n",
    "scaler = MinMaxScaler(feature_range=[5, 10])\n",
    "result = scaler.fit_transform(data) # 一步导出结果\n",
    "# 当x中的特征数量非常多时，fit会报错并表示，数据量太大计算不了\n",
    "# 这时候用partial_fit\n",
    "#scaler = scaler.partial_fit(data)\n",
    "\n",
    "# bonus 使用numpy来进行归一化\n",
    "import numpy as np\n",
    "x = np.array([[-1, 2], [-0.5, 6], [0, 10], [1, 18]])\n",
    "# 归一化\n",
    "x.min()\n",
    "x.min(axis=0)#跨列计算， 返回每一列的最小值\n",
    "x_nor = (x - x.min(axis=0)) / (x.max(axis=0) - x.min(axis=0))\n",
    "# 逆转归一化\n",
    "x_return = x_nor * (x.max(axis=0) - x.min(axis=0)) + x.min(axis)\n",
    "\n",
    "######################\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler() #实例化\n",
    "scaler.fit(data)# 本质是生成均值和方差\n",
    "scaler.mean_#两列的平均、\n",
    "scaler.var_#两列的方差\n",
    "s_std = scaler.transform(data)# 通过接口导出结果\n",
    "s_std.mean()#导出数组，查看均值\n",
    "s_std.std()# 查看方差\n",
    "scaler.fit_transform(data)#一步达成结果\n",
    "scaler.inverse_transform(s_std)# 逆转结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把数据改造成上课可用数据\n",
    "import pandas as pd\n",
    "data = pd.read_csv('data/data2138/train.csv')\n",
    "data = data.loc[:, ['Age', 'Sex', 'Embarked', 'Survived']]                  \n",
    "data.loc[data.loc[:, 'Survived'] == 1, 'Survived'] = 'yes'\n",
    "data.loc[data.loc[:, 'Survived'] == 0, \"Survived\"] = \"no\"\n",
    "for i in range(len(data.loc[:, 'Survived'])):\n",
    "    if i % 5 == 0:\n",
    "        data.loc[:, \"Survived\"].values[i] = \"unknown\"\n",
    "        #print(data.loc[:, \"Survived\"]) series\n",
    "        \n",
    "######################\n",
    "# 填补缺失值\n",
    "data.loc[:, 'Age']\n",
    "data.loc[:, 'Age'].values.reshape(-1,1).shape # (891, 1)\n",
    "Age = data.loc[:, 'Age'].values.reshape(-1, 1)\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp_mean = SimpleImputer()\n",
    "imp_median = SimpleImputer(strategy=\"median\")\n",
    "imp_0 = SimpleImputer(strategy=\"constant\", fill_value=0)\n",
    "\n",
    "imp_mean = imp_mean.fit_transform(Age)\n",
    "imp_median = imp_median.fit_transform(Age)\n",
    "imp_0 = imp_0.fit_transform(Age)\n",
    "\n",
    "# 为了不出现小数，用中值比较好\n",
    "data.loc[:, 'Age'] = imp_median\n",
    "\n",
    "# 使用众数\n",
    "Embarked = data.loc[:, 'Embarked'].values.reshape(-1, 1)\n",
    "imp_mode = SimpleImputer(strategy=\"most_frequent\")\n",
    "data.loc[:, \"Embarked\"] = imp_mode.fit_transform(Embarked)\n",
    "# numpy pandas更好的解决\n",
    "data.loc[:, 'Age'] = data.loc[:, 'Age'].fillna(data.loc[:, \"Age\"].mean())\n",
    "data.dropna(axis=0, inplace=True)\n",
    "# axis = 0 删除所有有缺失值的行，axis = 1删除所有有缺失值的列\n",
    "#inplace是在原数组修改，默认为false\n",
    "#data = data.dropna(axis = 0, inplace=False)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理分类型特征，编码与哑变量\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "y = data.iloc[:, -1]#要输入的是标签，不是特征矩阵，所以允许一维\n",
    "le = LabelEncoder()\n",
    "le = le.fit(y)\n",
    "label = le.transform(y)\n",
    "label# 生成了0 1 2\n",
    "#data.iloc[:, -1] = LabelEncoder().fit_transform(data.iloc[:, -1])\n",
    "\n",
    "# 特征专用，不能导入一维数组\n",
    "#categories对应于labelencoder的接口classes_,一摸一样的功能#每个类别里面有什么\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "data_ = data.copy()\n",
    "data_.head()\n",
    "#OrdinalEncoder().fit(data_.iloc[:, 1:-1]).categories_#每个类别里面有什么\n",
    "data.iloc[:, 1:-1] = OrdinalEncoder().fit_transform(data_.iloc[:, 1:-1])\n",
    "data_.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 哑变量\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "X = data.iloc[:, 1:-1]\n",
    "# 自动找一个特征中有几类\n",
    "enc = OneHotEncoder(categories=\"auto\").fit(X)\n",
    "result = enc.transform(X).toarray()# 稀疏矩阵变成数组，稀疏矩阵是只有0 1\n",
    "# OneHotEncoder(catrgories=\"auto\").fit_transform(X)。toarray()\n",
    "result#五列是因为性别中有两列，舱门中有三列\n",
    "#依然可以还原\n",
    "pd.DataFrame(enc.inverse_transform(result))\n",
    "# 找到类名\n",
    "enc.get_feature_names()\n",
    "\n",
    "#axis=1是跨行，左右相连,拼接\n",
    "newdata = pd.concat([data, pd.DataFrame(result)], axis=1)\n",
    "newdata.head()\n",
    "#删除原先的特征\n",
    "newdata.drop(['Sex', 'Embarked'], axis=1, inplace=True)\n",
    "newdata.columns = ['Age', 'Survived', 'Female', 'Male', 'Embarked_C', 'Embarked_Q', 'Embarked_S']\n",
    "\n",
    "newdata.head()\n",
    "# 标签可以是哑变量，但是在分类里面没什么必要\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#连续性特征：二值化和分段\n",
    "data_2 = data.copy()\n",
    "from sklearn.preprocessing import Binarizer\n",
    "X = data_2.iloc[:, 0].values.reshape(-1, 1)\n",
    "#有特征就不接受一维数组，要升维\n",
    "transformer = Binarizer(threshold=30).fit_transform(X)\n",
    "transformer\n",
    "data_2.iloc[:, 0] = transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#失踪了和阵亡差不多把\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "X = data.iloc[:, 0].values.reshape(-1, 1)\n",
    "#有几个特征返回几列，这一列里面有不同的类别箱子，等宽\n",
    "est = KBinsDiscretizer(n_bins=3, encode=\"ordinal\", strategy=\"uniform\")\n",
    "\n",
    "#查看转换后分的箱\n",
    "est.fit_transform(X).ravel()\n",
    "#放到集合里看看\n",
    "set(est.fit_transform(X).ravel())\n",
    "\n",
    "est = KBinsDiscretizer(n_bins=3, encode='onegot')\n",
    "est.fit_transform(X).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#特征工程\n",
    "import pandas as pd\n",
    "data = pd.read_csv('data/data11686/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()\n",
    "X = data.iloc[:, 1:]\n",
    "y = data.iloc[:, 0]\n",
    "X.shape\n",
    "#方差过滤\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "selector = VarianceThreshold()\n",
    "x_var0 = selector.fit_transform(X)\n",
    "x_var0.shape\n",
    "import numpy as np\n",
    "X.var()#所有列的方差\n",
    "np.median(X.var().values)# 方差中位数\n",
    "X_fsvar = VarianceThreshold(np.median(X.var().values)).fit_transform(X)\n",
    "X_fsvar.shape# 方差小的全砍掉\n",
    "# 若特征是伯努利随机变量，假设p = 0.8,即二分类特征中某种分类占到80%以上的时候删除特征\n",
    "X_bvar = VarianceThreshold(.8 * (1 - .8)).fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对比knn和随机森林，knn遍历会超级慢\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.neighbors import  KNeighborsClassifier as KNN\n",
    "from sklearn.model_selection import  cross_val_score\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import  chi2#卡方\n",
    "x = data[:, 1:]\n",
    "y = data[:, 0]\n",
    "x_fsvar = varianceThreshold(np.median(x.var().values)).fit_transform(x)\n",
    "# 中位数过滤后\n",
    "cross_val_score(KNN(),x, y, cv=5).mean()\n",
    "%%timeit\n",
    "#计算cell运算时间，是正常时间的7倍\n",
    "# 过滤后，结果还提高了，时间还减少了，所以对于knn，算法好用\n",
    "\n",
    "#随机森林非常快，几百倍差距\n",
    "#方差 过滤后，没什么影响\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.neighbors import  KNeighborsClassifier as KNN\n",
    "from sklearn.model_selection import  cross_val_score\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import  chi2#卡方\n",
    "#假设在这里我知道我需要300 个特征\n",
    "X_fschi = SelectKBest(chi2, k = 300).fit_transform(X_fsvar, y)\n",
    "X_fschi.shape\n",
    "cross_val_score(RFC(n_estimators=10, random_state=0),X_fschi, y, cv=5).mean()\n",
    "# 模型效果降低了，说明K设置小了，有一些有效特征被删除了\n",
    "# 选取超参数k,一般k越多，分数越高\n",
    "#求卡方和p值\n",
    "chivalue, pvalues_chi = chi2(X_fsvar, y)\n",
    "# 卡方值很难界定边界。p值可以看和0.05的关系\n",
    "#消除p大于0。05的就行\n",
    "# chivalue.shape[0]是特征总数， true是1， false是0\n",
    "k = chivalue.shape[0] - (pvalues_chi > 0.05).sum()\n",
    "\n",
    "# 进行F检验\n",
    "from sklearn.feature_selection import f_classif\n",
    "F, pvalues_f = f_classif(X_fsvar, y)\n",
    "# 操作同上卡方检验\n",
    "\n",
    "#互信息法\n",
    "from sklearn.feature_selection import mutual_info_classif as MIC\n",
    "result = MIC(X_fsvar, y)# 得到互信息量的估计,所有都在0 1之间\n",
    "k = result.shape[0] - sum(result <= 0)\n",
    "\n",
    "#(result == 0).sum()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ]
}