{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.6)# 簇的方差是0.6\n",
    "plt.scatter(X[:, 0],X[:, 1],c=y, s=50, cmap=\"rainbow\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contour绘制等高线[x, y], z, levels,xy是选填，xy是二维平面上横纵坐标的取值，二维平面的结构与z相同，往往通过numpy.meshgrid()，如果xy都是一维的，则z的结构必须为（len(y）,len(x)）,不填写则默认x-range（z.shape(1)),y=range(z.shape(0))\n",
    "# z 平面上所有点对应的高度，levels，不填默认显示所有的等高线，还有其她的一些设置\n",
    "# 首先获取样本构成的平面\n",
    "ax = plt.gca() # 获取当前的子图，如果不存在，则创建新的子图\n",
    "\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 制作网格\n",
    "# 画网格的思路是取到横纵坐标的最大值最小值，比如取到50个坐标点，再合并\n",
    "# 获取平面上两条坐标轴的最大值和最小值\n",
    "#xlim = ax.get_xlim()\n",
    "#ylim = ax.get_ylim()# 默认是0 1之间\n",
    "# 找一些中间dian \n",
    "axisx = np.linspace(xlim[0], xlim[1], 30)\n",
    "axisy = np.linspace(ylim[0], ylim[1], 30)\n",
    "# 我们将使用这里形成的二维数组作为我们contour函数中的x和y\n",
    "# 使用meshgrid函数将两个一维向量转换为特征矩阵\n",
    "# 核心是两个特征向量的广播，以便获得y.shape * x.shape这么多个坐标点的横坐标和纵坐标\n",
    "axisy, axisx = np.meshgrid(axisy, axisx)\n",
    "axisx.shape # 30 * 30\n",
    "# vstack能够将多个结构一致的一维数组按照行堆叠起来， ravel()是降维函数\n",
    "# xy是已经形成的网格，他是遍布再整个画布上的密集的点\n",
    "xy = np.vstack([axisx.ravel(), axisy.ravel()]).T\n",
    "xy.shape# 形成了900个点了\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建模，通过fit计算出相应的决策边界\n",
    "clf = SVC(kernel=\"linear\").fit(X, y)\n",
    "Z = clf.decision_function(xy).reshape(axisx.shape)\n",
    "# 重要接口decisionfunction，返回每个输入的样本所对应的到决策边界的距离\n",
    "# 距离再转回axisx的结构，因为画图函数要求z和xy结构相同\n",
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0],X[:, 1],c=y, s=50, cmap=\"rainbow\")\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "# 现在画决策边界\n",
    "# 画决策边界和平行于决策边界的超平面\n",
    "ax.contour(axisx, axisy, Z\n",
    "            ,colors=\"k\"\n",
    "            ,levels=[-1, 0, 1]# 画三条等高线，分别是z为-1，z为0和z为1的三条线\n",
    "            ,alpha=0.5 # 控制透明度\n",
    "            ,linestyles=[\"--\", \"-\", \"--\"])\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#画不出来就合并一下,之前的某些因素影响了画图结果\n",
    "X, y = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.6)# 簇的方差是0.6\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "axisx = np.linspace(xlim[0], xlim[1], 30)\n",
    "axisy = np.linspace(ylim[0], ylim[1], 30)\n",
    "# 我们将使用这里形成的二维数组作为我们contour函数中的x和y\n",
    "# 使用meshgrid函数将两个一维向量转换为特征矩阵\n",
    "# 核心是两个特征向量的广播，以便获得y.shape * x.shape这么多个坐标点的横坐标和纵坐标\n",
    "axisy, axisx = np.meshgrid(axisy, axisx)\n",
    "xy = np.vstack([axisx.ravel(), axisy.ravel()]).T\n",
    "clf = SVC(kernel=\"linear\").fit(X, y)\n",
    "Z = clf.decision_function(xy).reshape(axisx.shape)\n",
    "plt.scatter(X[:, 0],X[:, 1],c=y, s=50, cmap=\"rainbow\")\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "# 现在画决策边界\n",
    "# 画决策边界和平行于决策边界的超平面\n",
    "ax.contour(axisx, axisy, Z\n",
    "            ,colors=\"k\"\n",
    "            ,levels=[-1, 0, 1]# 画三条等高线，分别是z为-1，z为0和z为1的三条线\n",
    "            ,alpha=0.5 # 控制透明度\n",
    "            ,linestyles=[\"--\", \"-\", \"--\"])\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z 本身是输入的样本到决策边界的距离，而contour函数中的level其实是输入了这个距离\n",
    "plt.scatter(X[:, 0],X[:, 1],c=y, s=50, cmap=\"rainbow\")\n",
    "plt.scatter(X[10, 0],X[10, 1],c=\"black\", s=50, cmap=\"rainbow\")\n",
    "# 取出来的点是黑色的那个\n",
    "# 画一画决策边界的距离\n",
    "distance = clf.decision_function(X[10].reshape(1, 2))\n",
    "plt.scatter(X[:, 0],X[:, 1], c=y, s=50, cmap=\"rainbow\")\n",
    "ax = plt.gca()\n",
    "ax.contour(axisx, axisy, Z\n",
    "            ,colors=\"k\"\n",
    "            ,levels=[distance]\n",
    "            ,alpha=0.5\n",
    "            ,linestyle=[\"--\"])\n",
    "# 最后把以上过程包装成函数\n",
    "# 重要的接口\n",
    "clf.predict(X)\n",
    "# 根据决策边界，对X中的样本进行分类，返回的模型为n_samples\n",
    "clf.score(X, y)\n",
    "# 返回给定测试数据和标签的平均准确度\n",
    "clf.support_vectors_\n",
    "#返回支持向量\n",
    "clf.n_support_\n",
    "# 返回每个类中支持向量的个数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推广到非线性的情况上\n",
    "from sklearn.datasets import make_circles\n",
    "X, y = make_circles(100, factor=0.1, noise=0.1)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=\"rainbow\")\n",
    "plt.show()\n",
    "#直接画决策分界分不太好\n",
    "# 怎么办呢，这时候定义一个新的维度R进行升维\n",
    "# 求X的平方和，矩阵\n",
    "print(X.shape)# 100, 2\n",
    "r = np.exp(-(X ** 2).sum(1))\n",
    "print(r.shape) # （100，）\n",
    "# sum(0) (2,)是对列相加，sum(1)是对每一行进行相加\n",
    "rlim = np.linspace(min(r), max(r), 100)\n",
    "\n",
    "from mpl_toolkits import mplot3d\n",
    "# elev表示上下旋转的角度\n",
    "#azim表示平行旋转的角度\n",
    "def plot_3D(elev=30, azim=30, X=X, y=y):\n",
    "    ax = plt.subplot(projection=\"3d\")\n",
    "    ax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=50, cmap=\"rainbow\")# 颜色y，尺寸50，cmap彩虹色\n",
    "    ax.view_init(elev=elev, azim=azim)# 自动调整旋转角度\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "    ax.set_zlabel(\"r\")\n",
    "    plt.show()\n",
    "plot_3D()\n",
    "#升维是种核变换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#案例：如何选取最佳核函数\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_circles, make_moons, make_blobs, make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "datasets = [\n",
    "    make_moons(n_samples=n_samples, noise=0.2, random_state=0),\n",
    "    make_circles(n_samples=n_samples,noise=0.2, factor=0.5, random_state=1),\n",
    "    make_blobs(n_samples=n_samples, centers=2, random_state=5),\n",
    "    make_classification(n_samples=n_samples, n_features=2, n_informative=2,n_redundant=0, random_state=5)\n",
    "    # 分类指定两个特征，带信息的2个，不带信息的0个（噪音）\n",
    "]\n",
    "Kernel = [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\n",
    "\n",
    "# 四个数据集分别是什么样子\n",
    "for X, Y in datasets:\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=Y, s=50, cmap=\"rainbow\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = len(datasets)\n",
    "ncols = len(Kernel) + 1\n",
    "# 同时还要观察本身的情况\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(20, 16))\n",
    "# 开始进行循环\n",
    "# list(enumerate(datasets))== [*enumerate(datasets)]\n",
    "#打开后是[(索引，array([特征矩阵X],[标签Y]))]\n",
    "# 第一层再不同的数据集中循环\n",
    "for ds_cnt, (X, Y) in enumerate(datasets):\n",
    "    # 首先画第一列，是原始数据分布\n",
    "    ax = axes[ds_cnt, 0]\n",
    "    if ds_cnt == 0:\n",
    "        ax.set_title(\"input data\")\n",
    "        # zorder 在画布的哪个图层上，越大越在上面， cmap一种颜色，edge是散点的边线颜色\n",
    "        ax.scatter(X[:, 0], X[:, 1], c=Y, zorder=10, cmap=plt.cm.Paired, edgecolors=\"k\")\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "\n",
    "    # 第二层循环，再不同的核函数中进行循环\n",
    "    # 从图像的第二列开始，一个个填充分类结果\n",
    "    for est_idx, kernel in enumerate(Kernel):\n",
    "\n",
    "        # 定义子图位置\n",
    "        ax = axes[ds_cnt, est_idx + 1] # 从0， 1位置开始填\n",
    "        # 建模,并且拿到分数\n",
    "        clf = SVC(kernel=kernel, gamma=2).fit(X, Y)\n",
    "        score = clf.score(X, Y)\n",
    "        #绘制图像本身分布的散点图\n",
    "        ax.scatter(X[:, 0], X[:, 1], c=Y, zorder=10, cmap=plt.cm.Paired, edgecolors=\"k\")\n",
    "        # 绘制支持向量\n",
    "        ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=50,\n",
    "                    facecolors=\"none\", zorder=10, edgecolors=\"k\")\n",
    "        # facecolors是透明的\n",
    "        # 绘制边界\n",
    "        x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "        y_min, y_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "\n",
    "        # np.mgrid,合并了np.linspace和np.meshgrid的用法\n",
    "        # 一次性使用最大值和最小值生成网格\n",
    "        # 表示为【起止值，初始值， 步长】\n",
    "        # 如果步长是复数， 则其整数部分就是初始值和结束值之间创建的点的数量，并且结束值被包含在内\n",
    "        # j 就是复数的虚部,这里代表取200个整数，并且最后一个值也被取到，左右都是闭区间\n",
    "        XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\n",
    "        # np.c_,类似于np.vstack的功能,计算所有点到决策边界的距离，令距离为z\n",
    "        Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()]).reshape(XX.shape)\n",
    "        # 填充等高线不同区域的颜色, z 大于0 一种颜色，  z小于0 另一种颜色\n",
    "        ax.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired)\n",
    "        # 绘制等高线\n",
    "        ax.contour(XX, YY, Z, colors=[\"k\", \"k\", \"k\"], linestyles=[\"--\", \"-\", \"--\"],\n",
    "                    levels=[-1, 0, 1])# 把到决策边界为-1 0 1的等高线连接起来\n",
    "        # 设定坐标轴为不显示\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        # 将标题放在第一行的顶上\n",
    "        if ds_cnt == 0:\n",
    "            ax.set_title(kernel)\n",
    "\n",
    "        # 为每张图添加分类的分数,横坐标的位置，纵坐标的位置，要填写的分数，并且不显示0， 大小15\n",
    "        ax.text(0.95, 0.06, (\"%.2f\" % score).lstrip(\"0\")\n",
    "                ,size=15\n",
    "                ,bbox=dict(boxstyle=\"round\", alpha=0.8, facecolor=\"white\")\n",
    "                # 为分数添加一个白底的格子作为底色\n",
    "                ,transform=ax.transAxes# 确定文字所对应的坐标轴，就是ax子图的坐标轴本身（文字可以自己设置的坐标轴，不过一般都用子图本身的）\n",
    "                ,horizontalalignment=\"right\" # 位于坐标轴的什么方向\n",
    "                )\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "        \n",
    "\n",
    "# 结果显示，应该先rbf跑，之后再用别的，但是每个核函数都有自己的缺陷，下面给个例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 乳腺癌数据集\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from time import time \n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "X.shape# 二维\n",
    "np.unique(y)# 结果是两个标签\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=420)\n",
    "Kernel = [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\n",
    "for kernel in Kernel:\n",
    "    time0 = time()\n",
    "    clf = SVC(kernel=kernel\n",
    "            ,gamma=\"auto\"#\n",
    "            #, degree=1# 多项式核函数的次数，默认是3\n",
    "            ,cache_size=5000 # 可以允许使用多大的内存进行计算 MB\n",
    "            ).fit(x_train, y_train)\n",
    "    print(\"the accuracy under kernel %s is %f\" % (kernel,clf.score(x_valid, y_valid)))\n",
    "    print(datetime.datetime.fromtimestamp(time()-time0).strftime(\"%M:%S:%f\"))\n",
    "    # time()现在的时间浮点数，timestamp把浮点数转换成真正的时间\n",
    "    # strftime 转换为时间\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里看一看为什么为什么高斯核函数不太haoyong\n",
    "import pandas as pd\n",
    "data = pd.DataFrame(X)\n",
    "data.describe([0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.99])\n",
    "# 量纲严重不统一，有一些偏态问题，不是正态分布的\n",
    "# 我们重新搞一搞分布和量纲\n",
    "# 标准化\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X = StandardScaler().fit_transform(X)\n",
    "data = pd.DataFrame(X)\n",
    "data.describe([0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.99])\n",
    "#都是无量纲化加上归一化了\n",
    "# 这时候rbf涨了47的准确度\n",
    "\n",
    "#结论：线性核尤其是多项式核函数高次项时计算非常缓慢\n",
    "#rbf核函数不擅长处理量纲不统一的数据集\n",
    "# 参数进行交叉验证和学习曲线进行学习\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "time0 = time()\n",
    "gamma_range = np.logspace(-10, 1, 20)\n",
    "coef0_range = np.linspace(0, 5, 10)\n",
    "param_grid = dict(gamma = gamma_range\n",
    "                    ,coef0 = coef0_range)\n",
    "# 随机分为5份，其中30%是测试集\n",
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.3, random_state=420)\n",
    "grid = GridSearchCV(SVC(kernel = \"poly\", degree=1, cache_size=5000), param_grid=param_grid, cv=cv)\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(\"the best parameters are %s with a score of %0.5f\" % (grid.best_params_, grid.best_score_))\n",
    "print(datetime.datetime.fromtimestamp(time()-time0).strftime(\"%M:%S:%f\"))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对权重进行设置\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.datasets import make_blobs\n",
    "# 创建不均衡的数据\n",
    "class_1 = 500 # 类别1有500个样本\n",
    "class_2 = 50 # 类别2有50个样本\n",
    "centers = [[0.0, 0.0], [2.0, 2.0]]# 设定两个类别的中心\n",
    "clusters_std = [1.5, 0.5]# 设定两个类别的方差，通常来说，样本量比较大的类别会更加松散\n",
    "X, y = make_blobs(n_samples=[class_1, class_2], \n",
    "                    centers=centers,\n",
    "                    cluster_std=clusters_std,\n",
    "                    random_state=0, shuffle=False)\n",
    "#看看数据集的样子\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=\"rainbow\",s=10)\n",
    "# 红色是少数类\n",
    "# 不设定class_weight\n",
    "clf = svm.SVC(kernel=\"linear\", C=1.0)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# 设定class_weight,少数类是10，多数类是1\n",
    "wclf = svm.SVC(kernel=\"linear\", class_weight={1:10})\n",
    "wclf.fit(X, y)\n",
    "# 分别打分\n",
    "clf.score(X, y)\n",
    "wclf.score(X, y)\n",
    "\n",
    "# 样本均衡后准确率下降了\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 和之前一样的等高线画法\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=\"rainbow\", s=10)\n",
    "ax = plt.gca()\n",
    "# 获取当前子图，如果不存在 ，就创建新的子图\n",
    "\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "\n",
    "xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "YY, XX = np.meshgrid(yy, xx)\n",
    "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "\n",
    "# 第二步找出样本点到决策边界的距离\n",
    "Z_clf = clf.decision_function(xy).reshape(XX.shape)\n",
    "a = ax.contour(XX, YY, Z_clf, color=\"black\", levels=[0], alpha=0.5, linestyles=[\"-\"])\n",
    "\n",
    "Z_wclf = wclf.decision_function(xy).reshape(XX.shape)\n",
    "b = ax.contour(XX, YY, Z_wclf, color=\"red\", levels=[0], alpha=0.5, linestyles=[\"-\"])\n",
    "\n",
    "# 第三步，画网图\n",
    "plt.legend([a.collections[0], b.collections[0]],[\"non weighted\", \"weighted\"], loc=\"upper right\")\n",
    "plt.show()\n",
    "\n",
    "# a.collections返回一个惰性对象，返回一个列表里面存一条线\n",
    "#a.collections[0] 取出来线的对象\n",
    "# plt.legend([对象列表], [图例列表], loc)\n",
    "# 只要对象和图里列表相对应，就可以i显示出图例\n",
    "\n",
    "\n",
    "# 上面的应该用没有样本均衡的对象进行描绘，之后可以改下，做完样本均衡后，可能是分配了少数类，但是可能误杀了很多的多数类，可能模型整体的精确率下降了\n",
    "# 但是有些时候把多数类判断为少数类是对的，比如判断坏人\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 混淆矩阵的学习\n",
    "# 所有判断正确并确实为1的样本/所有被判断为1的样本\n",
    "# 对于没有class_weight， 没有做样本平衡的灰色决策边界来说：\n",
    "(y[y == clf.predict(X)] == 1).sum() / (clf.predict(X) == 1).sum()\n",
    "#y == clf.predict(X) 返回一堆true false，之后带入矩阵就是一堆1 0\n",
    "#true代表1 ，false代表0\n",
    "(y[y == wclf.predict(X)] == 1).sum() / (wclf.predict(X) == 1).sum()\n",
    "# 不带权重是0.71\n",
    "# 带权重的是0.51\n",
    "# 所以p值低的话可能是要不计代价的找到少数类\n",
    "# 当判断多数类错误的成本高的时候（比如大众汽车召回），我们会追求搞的精确度\n",
    "\n",
    "\n",
    "# recall召回率\n",
    "# 所有判断正确并确实为1 的样本 / 全部真实为1 的样本\n",
    "(y[y == clf.predict(X)] == 1).sum() / (y==1).sum()\n",
    "\n",
    "(y[y == wclf.predict(X)] == 1).sum() / (y == 1).sum()\n",
    "# 不计代价追求少数类，要看recall\n",
    "# 而召回率和精确度是此消彼长的，需要根据业务的需要进行两者的选择\n",
    "\n",
    "# 特异度\n",
    "(y[y== clf.predict(X)] == 0).sum() / (y==0).sum()\n",
    "(y[y == wclf.predict(X)] == 0).sum() / (y == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn 中的混淆矩阵\n",
    "# metrics里面有\n",
    "# 概率和阈值，比如阈值是0.5， 大于0.5 的概率就是阳性\n",
    "from sklearn.linear_model import LogisticRegression as LogiR\n",
    "clf_lo = LogiR().fit(x_, y_)\n",
    "prob = clf_lo.predict_proba(X_)\n",
    "#prob代表十一个样本对应是0和1的概率 \n",
    "# 将样本和概率放到一个dataframe中\n",
    "import pandas as pd\n",
    "prob = pd.DataFrame(prob)\n",
    "prob.columns = [\"0\", \"1\"]\n",
    "# 手动调节阈值，来改变我们的模型效果\n",
    "for i in range(prob.shape[0]):\n",
    "    if prob.loc[i, \"1\"] > 0.5:\n",
    "        prob.loc[i, \"pred\"] = 1\n",
    "    else:\n",
    "        prob.loc[i, \"pred\"] = 0\n",
    "prob[\"y_true\"] = y_\n",
    "## 根据1排序，不要逆序\n",
    "prob = prob.sort_values(by=\"1\", ascending=False)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix as CM, precision_score as P, recall_score as R\n",
    "CM(prob.loc[:, \"y_true\"], prob.loc[:, \"pred\"], labels=[1,0])# 少数类写在前面\n",
    "P(prob.loc[:, \"y_true\"], prob.loc[:, \"pred\"], labels=[1,0])\n",
    "R(prob.loc[:, \"y_true\"], prob.loc[:, \"pred\"], labels=[1,0])\n",
    "# 调节阈值不一定能够出现单调，就调调\n",
    "# 通常降低阈值升高recall，阈值是影响概率然后再影响结果的\n",
    "# 有概率需求就要最好用逻辑回归朴素贝叶斯等等，对于决策树的叶子节点里每个类别的样本就是概率，但是一个叶子只有一个标签概率就是1或者0了，无法调节了\n",
    "#svm也可以生成概率\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probability \n",
    "# 一般距离超平面越远的点归属的概率比较大\n",
    "#decision_function 是置信度，没有边界可以无限大，但是支持向量机也不像决策树一样可以算出概率，这时候有probability\n",
    "#probability默认不用，启用必须再fit之前，但是会减慢运算速度，启用后predict_proba predict_log_proba会生效\n",
    "#二分类可以用platt缩放，就是再decisionfunction生成的距离上进行sigmoid、压缩，ing附加交叉验证拟合，来生成类逻辑回归的svm分数\n",
    "# 多酚类有论文\n",
    "clf_proba = svm.SVC(kernel=\"linear\", C=1.0, prebability=True).fit(X, y)\n",
    "clf_proba.predict_proba(X)\n",
    "# 生成的各类标签下的概率\n",
    "# 550, 2\n",
    "clf_proba.decision_function(X)\n",
    "# 550,\n",
    "# 生成距离，分类通过正负来分别\n",
    "# 但是很慢，用的platt平滑很慢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制roc曲线\n",
    "# 首先要有概率和阈值，求出混淆矩阵\n",
    "# 首先看如何从混淆矩阵中取得recall和fpr\n",
    "cm = CM(prob.loc[:, \"y_true\"], prob.loc[:, \"pred\"], labels=[1, 0])\n",
    "# FPR， 被我们预测错误的0占所有真正为0的样本的比例\n",
    "cm[1, 0] / cm[1,:].sum()\n",
    "# recall\n",
    "cm[0, 0] / cm[0, :].sum()\n",
    "# 开始画\n",
    "#概率 clf_proba.predict_proba(X)[:, 1]我的类别1下的概率\n",
    "\n",
    "\n",
    "\n",
    "# 阈值，每个一阈值对应一次循环，每一次循环，都要有一个混淆矩阵，要有一组假证率和recall\n",
    "# np.linspace(min, max, 55,endpoint=false) 最大值最小值都是闭区间,endpoint=false控制不要取到最大值\n",
    "# if i > 概率最大值，返回1\n",
    "recall = []\n",
    "FPR = []\n",
    "\n",
    "probrange = np.linspace(clf_proba.predict_proba(X)[:, 1].min(), clf_proba.predict_proba(X)[:, 1].max(), 55)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix as CM, recall_score as R\n",
    "for i in probrange:\n",
    "    y_predict[]\n",
    "    for j in range(X.shape[0]):\n",
    "        if clf_proba.predict_proba(X)[j, 1] > i:\n",
    "            y_predict.append(1)\n",
    "        else:\n",
    "            y_predict.append(0)\n",
    "    cm = CM(y, y_predict, labels=[1, 0])\n",
    "    recall.append(cm[0, 0] / cm[0, :].sum())\n",
    "    FPR.append(cm[1, 0] / cm[1,:].sum())\n",
    "\n",
    "recall.sort()\n",
    "FPR.sort()\n",
    "\n",
    "\n",
    "plt.plot(FPR, recall, c='red')\n",
    "plt.plot(probrange+0.05, probrange+0.05, c=\"black\", linestyles=\"--\")\n",
    "plt.show()\n",
    "# 画出来roc曲线，不同的阈值得到不同的结果\n",
    "# roc曲线是取少数类的时候误伤多数类的情况，所以希望recall(纵坐标大)，假证率（横坐标小），也就是图像在左上方\n",
    "# 中间的虚线代表没捕捉一个少数类，就会误伤一个多数类，一般为凸起来的形状\n",
    "# 越接近左下方凸也没事，只要把阈值反向设置就是转到右上方了，最害怕在虚线附近，根本没办法调整\n",
    "# auc面积是曲线下方的面积\n",
    "\n",
    "#经常使用的类\n",
    "# 自己做的曲线\n",
    "sklearn.metrics.roc_curve()\n",
    "#参数 y_true 真实标签 ，，y_score置信度分数，可以是正类样本的概率值，或置信度分数，或者decision_function返回的距离\n",
    "#pos_label表示为认为是正类样本的类别\n",
    "# sample_weight表示样本的权重\n",
    "# drop_internediate舍弃一些roc曲线上不显示的阈值点，这对于计算一个比较轻量的roc曲线很重要\n",
    "# 返回RPR, recall, 阈值-o\n",
    "from sklearn.metrics import roc_curve\n",
    "FPR, recall, thresholds = roc_curve(y, clf_proba.decision_function(X),pos_label = 1)\n",
    "# 都算出来，置信度的阈值\n",
    "from sklearn.metrics import roc_auc_score as AUC\n",
    "AUC(y, clf_proba.decision_function(X))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(FPR, recall, color=\"red\",\n",
    "        label=\"roc curve (are %0.2f)\" area)\n",
    "plt.plot([0, 1], [0, 1], color=\"black\", lw=lw, linestyle=\"--\")\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel(\"fpr\")\n",
    "plt.ylabel(\"recall\")\n",
    "plt.title('roc')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# 利用roc取得最佳阈值\n",
    "#recall增长，fpr最小\n",
    "max((recall - FPR).tolist())\n",
    "# list.index(zuidazhi) 返回这个最大值在list中的索引\n",
    "# 返回相差最大的索引\n",
    "maxindex = (recall - FPR).tolist().index(max(recall - FPR))\n",
    "# 43\n",
    "thresholds[maxindex] # decision_function返回置信度\n",
    "# 加入图中\n",
    "plt.scatter(FPR[maxindex], recall[maxindex], c=\"black\", s=30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用SVC的考虑,多分类问题，创建、处理多分类问题\n",
    "# decision_function_shape性质，自学\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#案例，预测明天是否会下雨\n",
    "# 数据预处理与特征工程的思路\n",
    "#######################非常重要\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pd.read_csv(r\"~/data/data29973/weatherAUS.csv\")\n",
    "#index_col参数是第几列当作索引\n",
    "weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = weather.iloc[:, :-1]\n",
    "Y = weather.iloc[:, -1]\n",
    "# 数据过大，就跟着敲一遍好了\n",
    "X.shape\n",
    "# 我们要有不同的填补策略\n",
    "# 看标签是否完整\n",
    "Y.isnull().sum()\n",
    "# 看标签\n",
    "np.unique(Y)\n",
    "# 在实际工作中，要先区分测试集和验证集之后再进行预处理，这个顺序非常关键\n",
    "x_train, y_train, x_valid, y_valid = train_test_split(X, Y, test_size=0.3, random_state=420)\n",
    "# 索引变乱，恢复索引\n",
    "for i in [x_train, x_valid, y_train,y_valid]:\n",
    "    i.index = range(i.shape[0])\n",
    "# 查看是否有不平衡的问题\n",
    "y_train.value_counts()\n",
    "\n",
    "#将标签编码\n",
    "from sklearn.preeprocessing import LabelEncoder # 标签专用\n",
    "encorder = LabelEncoder().fit(y_train)  #允许一维数据的输入，其他的编码类都不支持\n",
    "# 认识了yes是1， no是0\n",
    "\n",
    "# 使用训练集进行训练，然后再训练集和测试集分别进行transform\n",
    "y_train = pd.DataFrame(encorder.transform(y_train))\n",
    "y_valid = pd.DataFrame(encorder,transform(y_valid))\n",
    "# 如果测试集中出现了训练集中没出现的类别会报错，这时候要去重新建模\n",
    "# 最好先保存，避免之后出现问题还要重新运行模型\n",
    "y_train.to_csv(\"wenjiandizhi.wenjianming.csv\")\n",
    "\n",
    "#x_train.describe([0.01, 0.05,...])\n",
    "#看看数据,中间填上分位数，看看是不是偏态\n",
    "\n",
    "# 找到异常值\n",
    "# 首先找到异常值出现的频率，如果异常值只出现了一次，多半是人为错误，直接删除，出现多次，要找业务人员沟通。\n",
    "# 很大或者很小都可呢是异常值，比如负数的降雨量\n",
    "# 异常当作缺失等等\n",
    "\n",
    "# 先查看原始的数据结构\n",
    "x_train.shape\n",
    "x_valid.shape\n",
    "\n",
    "# 观察异常值是大量存在吗\n",
    "x_train.loc[x_train.loc[:, \"cloud9am\"] == 9, \"cloud9am\"]\n",
    "x_valid.loc[x_valid.loc[:, \"cloud9am\"] == 9, \"cloud9am\"]\n",
    "x_valid.loc[x_valid.loc[:, \"cloud3am\"] == 3, \"cloud3am\"]\n",
    "#删除策略的话，如果删除特征矩阵，一定要连着标签一起删除，所以特征矩阵的行和标签的行必须一一对应\n",
    "x_train = x_train.drop(index = 71737)\n",
    "y_train = y_train.drop(index = 71737)\n",
    "# 删除完毕后，观察一下\n",
    "x_train.shape\n",
    "#进行任何索引之后都要恢复索引\n",
    "for i in [x_train, x_valid, y_train,y_valid]:\n",
    "    i.index = range(i.shape[0])\n",
    "\n",
    "\n",
    "# 处理困难特征：日期, 很难放到算法里面\n",
    "x_trainc = x_train.copy()\n",
    "x_trainc.sort_values(by=\"location\")\n",
    "x_train.iloc[:, 0].value_counts()\n",
    "# 首先日期有重复，其次日期不是连续的，某一天会倾向于下雨吗，不是日期影响了下雨，而是这天的空气湿度等等影响的下雨\n",
    "# 只看日其没有影响，若把他当作连续型变量处理，算法会认为他是1-3000的数字，不会意识到这是日期\n",
    "# 日期转换为哑变量会特征爆炸\n",
    "# 可以删除\n",
    "x_train = x_train.drop([\"Date\"],axis=1)\n",
    "x_valid = x_valid.drop([\"Date\"],axis=1)\n",
    "\n",
    "# 可能日期是时间序列分析！！！但是时间序列需要是同一个地区的时间预测，但我们的数据不单调不唯一，经过抽样连续都不是，混在多个地点里面的每个地点的一小段时间\n",
    "# 日期的重复是根据地点不同会有重复\n",
    "\n",
    "# 在换一种思路，我们处理的列于列的关系，是否可以把今天天气影响明天天气作为一个特征\n",
    "# 发现今天的降雨量，可以转换为今天是否会下午，然后看不同的天会不会下雨，就把样本对标签的影响转换为特征对标签的影响\n",
    "x_train[\"rainfall\"].head()\n",
    "# 这里认为大于1毫米是下雨\n",
    "x_train[\"Rainfall\"].isnull().sum()# 33\n",
    "# 空值有33个\n",
    "# 假设没有下雨,\n",
    "# 通过降雨量， 分配今天是否会下雨\n",
    "x_train.loc[x_train[\"Rainfall\"] >= 1, \"RainToday\"] = \"Yes\"\n",
    "x_train.loc[x_train[\"Rainfall\"] < 1, \"RainToday\"] = \"No\"\n",
    "x_train.loc[x_train[\"Rainfall\"] == np.nan, \"RainToday\"] = np.nan\n",
    "x_valid.loc[x_valid[\"Rainfall\"] >= 1, \"RainToday\"] = \"Yes\"\n",
    "x_valid.loc[x_valid[\"Rainfall\"] < 1, \"RainToday\"] = \"No\"\n",
    "x_vaild.loc[x_valid[\"Rainfall\"] == np.nan, \"RainToday\"] = np.nan\n",
    "# 日期可能不会影响下雨，但是月份可能会影响, 我们提取出月份进行操作\n",
    "int(x_train.loc[0, \"Data\"].split(\"-\")) # split拆成列表\n",
    "# apply是对dataframe上的某一列进行处理的一个函数\n",
    "# lambda x 匿名函数，请在dataframe上这一列中的每一行帮我执行冒号后的命令# 循环很慢，避免循环\n",
    "x_train[\"Data\"] = x_train[\"Data\"].apply(lambda x:int(x.split(\"-\")[1]))\n",
    "x_train.loc[:, \"Data\"]\n",
    "# 全部改名 x_train.column = [...]\n",
    "x_train = x_train.rename(column={\"Data\":\"Mouth\"}) # 改一个\n",
    "\n",
    "x_valid[\"Data\"] = x_valid[\"Data\"].apply(lambda x:int(x.split(\"-\")[1]))\n",
    "x_valid.loc[:, \"Data\"]\n",
    "# 全部改名 x_train.column = [...]\n",
    "x_valid = x_valid.rename(column={\"Data\":\"Mouth\"}) # 改一个\n",
    "\n",
    "# 处理困难特征：地点\n",
    "# 地区的名字对算法来说没有影响，怎么表示地区对气候的影响呢\n",
    "x_train.loc[:, \"Location\"].value_counts().count()  # 总共有49个类别（地点）\n",
    "# 不是城市本身对天气的影响，而是气候对天气的影响\n",
    "# 查找地区澳大利亚主要有8种，爬虫！这时候爬城市的经纬度，把49个城市的经纬度，在一对比就找到了\n",
    "\n",
    "# 课程老师爬取了\n",
    "cityll = pd.read_csv(\"\",index_col=0)\n",
    "# 每个城市对应的经纬度，这些城市是澳大利亚地图上的城市\n",
    "city_climate = pd.read_csv(\"\")\n",
    "# 澳大利亚统计局做个每个城市对应的气候\n",
    "# 经纬度是个字符串，且最后一位是度数符号，切片,再转换为浮点数\n",
    "#float(cityll.loc[0, \"Latitude\"][:-1])\n",
    "cityll[\"Latitudenum\"] = cityll[\"Latitude\"].apply(lambda x:float(X[:-1]))\n",
    "cityll[\"Longitudenum\"] = cityll[\"Longitude\"].apply(lambda x:float(X[:-1]))\n",
    "cityll.loc[:, \"Latitudedir\"].value_counts() #所有的城市都是南半球\n",
    "# 澳大利亚整个都是南纬东经，所有经纬度的方向我们可以舍弃了\n",
    "# 取出城市名称和经度纬度的数字\n",
    "cityld = cityll.iloc[:, [0, 5, 6]]\n",
    "# 将气候加入\n",
    "cityld[\"climate\"] = city_climate.iloc[:, -1]\n",
    "# 就最后合并成一个表了\n",
    "# 查查气候\n",
    "citylld.loc[:, \"climate\"].value_counts()\n",
    "\n",
    "### 接下来操作趴下来的例子城市的数据，同训练集一样进行操作\n",
    "samplecityd.head() #包含城市，经度 维度\n",
    "\n",
    "# 计算地理上两点之间的距离，最近的城市就聚类到例子城市里\n",
    "# 公式不用管了\n",
    "from math import radians, sin, cos, acos\n",
    "cityld.loc[:, \"slat\"] = cityld.iloc[:, 1].apply(lambda x : radians(x))\n",
    "cityld.loc[:, \"slon\"] = cityld.iloc[:, 2].apply(lambda x : radians(x))\n",
    "samplecityd.loc[:, \"elat\"] = samplecityd.iloc[:, 1].apply(lambda x : radians(x))\n",
    "samplecityd.loc[:, \"elon\"] = samleccityd.iloc[:, 2].apply(lambda x : radians(x))\n",
    "# 把经纬度转换为弧度,用弧度表示经纬度\n",
    "import sys\n",
    "# 每个样本城市都运行一遍\n",
    "for i in range(samplecityd.shape[0]):\n",
    "    slat = citylld.loc[:, \"slat\"]\n",
    "    slon = citylld.loc[:, \"slon\"]\n",
    "    elat = samplecityd.loc[:, \"elct\"]\n",
    "    elon = samplecityd.loc[:, \"elon\"]\n",
    "    dist = 6371.01 * np.arccos(np.sin(slat)*np.sin(elat) + np.cos(slat)*np.cos(elat)*np.cos(slon.values - elon))\n",
    "    city_index = np.argsort(dist)[0]\n",
    "    # 每次计算后，取距离最近的城市，然后将最近的城市和城市对应的气候都匹配到samplecityd中\n",
    "    samplecityd.loc[i, \"closest_city\"] = citylld.loc[city_index, \"City\"]\n",
    "    samplecityd.loc[i, \"climate\"] = citylld.loc[city_index, \"climate\"]\n",
    "\n",
    "# 查看最后的结果，需要检查城市匹配是否正确\n",
    "samplecityd.head(300)\n",
    "# 查看气候的分布\n",
    "samplecityd[\"climate\"].value_counts()\n",
    "# 确认无误后，取出样本城市所对应的气候，并保存\n",
    "locafinal = samplecityd.iloc[:, [0, -1]]\n",
    "\n",
    "locafinal.head()\n",
    "# 就是气象站对应的气候两列\n",
    "locafinal.columns = [\"Location\", \"climate\"]\n",
    "# 在这里设定lacafinal的索引为地点，是为了之后进行map的匹配\n",
    "locafinal = locafinal.set_index(keys=\"location\")\n",
    "\n",
    "locafinal.to_csv(\".csv\")\n",
    "locafinal.head()\n",
    "\n",
    "# 接下来就可以利用气候代替气象站（城市）的名称\n",
    "# 这里可以使用map功能，map可以将特征中的值一一对应到我们设定的字典里，并用地点里的值代替样本中原本的值，我们在评分卡中曾经使用这个功能来用woe替换我们原本的特征的值\n",
    "\n",
    "# 训练集\n",
    "x_train.head()\n",
    "#确保匹配进入的气候字符串中不含有逗号，气候两边不含有空格\n",
    "# 我们使用re这个模块来消除逗号\n",
    "# re.sub(希望替换的值，希望被替换成的值，要操作的字符串)\n",
    "# x.strip()是去掉空格的函数\n",
    "\n",
    "\n",
    "\n",
    "# 把location替换成气候的是我们的map映射//\n",
    "import re\n",
    "x_train[\"location\"] = x_train[\"location\"].map(locafinal.iloc[:, 0]).apply(lambda x: re.sub(\", \", \"\", x.strip()))\n",
    "x_valid[\"location\"] = x_valid[\"location\"].map(locafinal.iloc[:, 0]).apply(lambda x: re.sub(\", \", \"\", x.strip()))\n",
    "# 改一下名字\n",
    "x_train = x_train.rename(columns={\"Location\":\"Climate\"})\n",
    "x_valid = x_valid.rename(cloumns={\"Location\":\"Climate\"})\n",
    "x_train.head()\n",
    "x_valid.head()\n",
    "\n",
    "# 处理分类型变量：缺失值\n",
    "# 实际生活中用算法填补缺失值比较少，一般都用众数(非连续)或者平均值（连续值）\n",
    "# 在实际工作中，训练集和测试集的缺失值都用训练集的众数或者平均值进行填写,因为可能测试集里只有一条数据没有特征，所以我们认为测试集和训练集有相同的分布\n",
    "# 但是其实训练集测试集分别填写自己的平均值也不会影响模型，也不会透露信息，但是不这样的原因如上\n",
    "#我们填补缺失值的类也需要实例化，fit和接口调用，然后可以执行fit后的结果分别填补测试集和训练集\n",
    "# 查看缺失值的情况\n",
    "x_train.isnull().mean()\n",
    "x_train.info()\n",
    "x_train.dtypes\n",
    "# 找出分类型的额特征\n",
    "cate = x_train.columns[x_train.dtypes == \"object\"].tolist()\n",
    "cate\n",
    "# 除了特征类型为object的特征们，还有虽然用数字表示，但是本质为分类特征的云层遮蔽程度\n",
    "cloud = [\"Cloud9am\", \"Cloud3pm\"]\n",
    "cate = cate + cloud\n",
    "cate\n",
    "# 对于分类型的特征，我们用众数进行填补\n",
    "from sklearn.impute import SimpleImputer #0.20, conda, pip\n",
    "\n",
    "si = SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")\n",
    "# 我们使用训练集数据来训练我们的填补器，本质是在生成训练集的众数\n",
    "si.fit(x_train.loc[:, cate])\n",
    "# 然后用训练集中的众数来同时填写训练集和测试集\n",
    "x_train.loc[:,cate] = si.transform(x_train.loc[:, cate])\n",
    "x_valid.loc[:,cate] = si.transform(x_train.loc[:, cate])\n",
    "x_train.head()\n",
    "x_valid.head()\n",
    "\n",
    "# 处理分类型变量：将分类型变量编码\n",
    "# 编码和填补缺失值一样，先fit模型，本质时将模型已有的特征转化为数字，然后transform分别在测试集和训练集上编码我们的特征矩阵\n",
    "# 如果在编码测试集出现错误可能是出现了训练集中没有的类别，这是可能是测试集中有错误值异常值，或者训练集中没有这个类别要重新进行调整模型\n",
    "# 将所有的分类行数据变量编码为数字，一个类别是一个数字（普通编码）\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "oe = OrdinalEncoder() #只允许二维以上的数据进行输入\n",
    "# 利用训练集进行fit\n",
    "oe = oe.fit(x_train.loc[:, cate])\n",
    "\n",
    "# 用结果来编码训练和测试特征矩阵\n",
    "# 在这里如果测试特征矩阵报错，就说测试集中出线了训练集中从未见过的类别\n",
    "x_train.loc[:, cate] = oe.transform(x_train.loc[:, cate])\n",
    "x_valid.loc[:, cate] = oe.transform(x_valid.loc[:, cate])\n",
    "\n",
    "x_train.loc[:, cate].head()\n",
    "x_valid.loc[:, cate].head()\n",
    "\n",
    "# 加入想做哑变量，可能支持向量机运行时间太长\n",
    "\n",
    "# 处理连续性变量：填补缺失值\n",
    "# 不用算法是算法是黑箱且算法太过于缓慢\n",
    "col = x_train.columns.tolist()\n",
    "for i in cate:\n",
    "    col.remove(cate)\n",
    "impmean = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n",
    "# 用训练集来fit模型\n",
    "impmean = impmean.fit(x_train)\n",
    "# 分别在训练集和测试集上进行均值填补\n",
    "x_train.loc[:, col] = impmean.transform(x_train.loc[:, col])\n",
    "x_valid.loc[:, col] = impmean.transform(x_valid.loc[:, col])\n",
    "x_train.head()\n",
    "x_valid.head()\n",
    "x_train.isnull().mean() # 检查一下\n",
    "### 连续行变量：无量纲化\n",
    "cate\n",
    "# 月份不能归一化\n",
    "col.remove(\"Month\")\n",
    "from sklearn.preprocessing import StandardScaler # 数据转换为均值为0，方差为1 的数据\n",
    "# 标准化不改变数据的分布，不会吧数据变成正态分布\n",
    "ss = StandardScaler()\n",
    "ss = ss.fit(x_train.loc[:, col])\n",
    "x_train.loc[:, col] = ss.transform(x_train.loc[:, col])\n",
    "x_valid.loc[:, col] = ss.transform(x_valid.loc[:, col])\n",
    "x_train.head()\n",
    "x_valid.head()\n",
    "# 建模与模型评估\n",
    "from time import time # 随时监控我们的模型的运行时间\n",
    "import datetime\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, recall_score\n",
    "#dateframe是二维结构,把她变成一维\n",
    "y_train = y_train.iloc[:, 0].ravel()\n",
    "y_valid = y_valid.iloc[:, 0].ravel()\n",
    "# 对支持向量机来说首先选择核函数\n",
    "times = time()\n",
    "for kernel in [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]:\n",
    "    clf = SVC(kernel = kernel\n",
    "                ,gamma=\"auto\"\n",
    "                ,degree=1\n",
    "                ,cache_size=5000 # 设定的内存使用量\n",
    "                ).fit(x_train, y_train)\n",
    "    result = clf.predict(x_valid) # 获取模型的预测结果\n",
    "    score = clf.score(x_valid, y_valid) # 返回准确度accuracy\n",
    "    recall = recall_score(y_valid, result) # 返回召回\n",
    "    auc = roc_auc_score(y_valid, clf.decision_function(x_valid)) # 第一个是y_true，第二个参数是我们的置信度/概率/分数，但不是result\n",
    "    print(\"%s 's testing accuracy %f, recall is %f, auc is %f\" % (kernel, score, recall, auc))\n",
    "    print(datetime.datetime.fromtimestamp(time() - times).strftime(\"%M:%S:%f\"))\n",
    "\n",
    "\n",
    "\n",
    "# 基本确定线性核函数, 接下来进行调参\n",
    "# 当我们想追求最高的recall时\n",
    "# 加上class_weight，模型好了很多\n",
    "# 在确定了线性核之后就使得权重更加倾向于少数类，来不计一切代价提升recall\n",
    "times = time()\n",
    "for kernel in [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]:\n",
    "    clf = SVC(kernel = kernel\n",
    "                ,gamma=\"auto\"\n",
    "                ,degree=1\n",
    "                ,cache_size=5000 # 设定的内存使用量\n",
    "                ,class_weight=\"balance\"\n",
    "                ).fit(x_train, y_train)\n",
    "    result = clf.predict(x_valid) # 获取模型的预测结果\n",
    "    score = clf.score(x_valid, y_valid) # 返回准确度accuracy\n",
    "    recall = recall_score(y_valid, result) # 返回召回\n",
    "    auc = roc_auc_score(y_valid, clf.decision_function(x_valid)) # 第一个是y_true，第二个参数是我们的置信度/概率/分数，但不是result\n",
    "    print(\"%s 's testing accuracy %f, recall is %f, auc is %f\" % (kernel, score, recall, auc))\n",
    "    print(datetime.datetime.fromtimestamp(time() - times).strftime(\"%M:%S:%f\"))\n",
    "# 再调整class_weight = {1:10} 注意这里得到的是类别1，权重10，隐藏了类别0：1这个比例\n",
    "# 随着recall无节制上升，我们的精确度下降了十分厉害，auc曲线面积还好，虽然我们的精确度很低，但是我们可以基本抓出来每一个少数类\n",
    "\n",
    "\n",
    "# 追求最高的准确率\n",
    "# 但是判断是否会下雨，光抓下雨不是很合理\n",
    "valuec = pd.Series(y_valid).value_counts()\n",
    "# 变成一维的数据\n",
    "valuec[0] / valuec.sum()\n",
    "# 我们的模型已经比全部判断为多数类的情况的模型情况要好了，\n",
    "# 比如在沙漠判断全部下雨准确率可能99%_\n",
    "# 查看模型的特异度\n",
    "from sklearn.metrics import confusion_matrix as CM\n",
    "clf = SVC(kernel = \"linear\"\n",
    "            ,gamma=\"auto\"\n",
    "            ,cache_size= 5000\n",
    "            ).fit(x_train, y_train)\n",
    "result = clf.predict(x_valid)\n",
    "cm = CM(y_valid, result, labels=(1, 0)) # 混淆矩阵输入真实值和预测值\n",
    "specificity = cm[1, 1] / cm[1, :].sum()\n",
    "specificity # 几乎所有的0都被判断正确了，还有不少1也被判断正确了\n",
    "#这时候对多数类偏向，会伤害少数类，而向少数类偏还会伤害多数类\n",
    "# 可以将class_weight稍微向少数类(1)移动，如果出现了跟高的准确率，则说明模型还没有到极限\n",
    "cm\n",
    "irange = np.linspace(0.01, 0.05, 10)\n",
    "for i in irange:\n",
    "    clf = SVC(kernel = \"linear\"\n",
    "            ,gamma=\"auto\"\n",
    "            ,cache_size= 5000\n",
    "            ,class_weight={1:1+i}\n",
    "            ).fit(x_train, y_train)\n",
    "    result = clf.predict(x_valid)\n",
    "    score = clf.predict(x_valid, y_valid)\n",
    "    recall = recall_score(y_valid, result)\n",
    "    auc = roc_auc_score(y_valid, clf.decision_function(x_valid))\n",
    "    print(\"under ratio 1:%f testing accuracy %f, recall is %f, recall is %f, auc is %f\" % (1+i,score,recall,auc))\n",
    "    print(datetime.datetime.fromtimestamp(time()-times).strftime(\"%M:%s:%f\"))\n",
    "# 不断细化    \n",
    "# 发现到了极限，我们只能更换在线性上更好的模型了\n",
    "# 比如逻辑回归,调节C值\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "logcif = LR(solver=\"liblinear\").fit(x_train, y_train)\n",
    "logcif = score(x_valid, y_valid)\n",
    "C_range = np.linspace(3, 5, 10)\n",
    "for C in C_range:\n",
    "    logcif = LR(solver=\"liblinear\",C=C).fit(x_train, y_train)\n",
    "    print(C, logcif.score(x_valid, y_valid))\n",
    "# 集成算法会调整的比较好，比如集成树\n",
    "#### 追求平衡的结果\n",
    "# 我们尝试调节线性核函数的c值进行提升\n",
    "import matplotlib.pyplot as plt\n",
    "C_range = np.linspace(0.01, 20, 20)\n",
    "\n",
    "recallall = []\n",
    "aucall = []\n",
    "scoreall = []\n",
    "for C in C_range:\n",
    "    times = time()\n",
    "    clf = SVC(kernel = \"linear\"\n",
    "                ,C=C\n",
    "                ,cache_size=5000 # 设定的内存使用量\n",
    "                ,class_weight=\"balance\"\n",
    "                ).fit(x_train, y_train)\n",
    "    result = clf.predict(x_valid)\n",
    "    score = clf.score(x_valid, y_valid)\n",
    "    recall = recall_score(y_valid, result)\n",
    "    auc = roc_auc_score(y_valid, clf.decision_function(x_valid))\n",
    "    recallall.append(recall)\n",
    "    aucall.append(auc)\n",
    "    scoreall.append(score)\n",
    "    print(\"\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(C_range, recallall, c=\"red\", label=\"recall\")\n",
    "plt.plot(C_range, aucall, c=\"black\", label=\"auc\")\n",
    "plt.plot(C_range, scoreall, c=\"orange\", label=\"accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 调整了c值还是没有质变，而且寻找c值会占用很大内存\n",
    "\n",
    "\n",
    "# 没什么调整的就花花auc的曲线，我们还是希望少数类高一点的\n",
    "from sklearn.metrics import roc_curve as ROC\n",
    "import matplotlib.pyplot as plt\n",
    "# 正样本是1\n",
    "FPR, Recall, thresholds = ROC(y_valid, clf.decision_function(x_valid), pos_label=1)\n",
    "area = roc_auc_score(y_valid, clf.decision_function(x_valid))\n",
    "plt.figure()\n",
    "plt.plot(FPR, Recall, color=\"red\", label=\"roc curve (area = %0.2f)\" % area)\n",
    "plt.plot([0, 1], [0, 1], color=\"black\", linestyle=\"--\")\n",
    "plt.xlim([-.0.05, 1.05])\n",
    "plt.ylim([-.0.05, 1.05])\n",
    "plt.xlabel(\"false positive rate\")\n",
    "plt.ylabel('recall')\n",
    "\n",
    "plt.title(\"receiver operating characteristic example\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# 以此模型为基础，我们来求解最佳阈值\n",
    "maxindex  = (Recall - FPR).tolist().index(max(Recall - RPR))\n",
    "thresholds[maxindex]\n",
    "\n",
    "# 基于选出的最佳阈值，我们人为确定y_predict, 并确定在这个阈值下的recall和准确度的值\n",
    "from sklearn.metrics import accuracy_score as AC\n",
    "times = time()\n",
    "clf = SVC(kernel=\"linear\", C=3.16, cache_size=5000\n",
    "            ,class_weight=\"balance\"\n",
    "            ).fit(x_train, y_train)\n",
    "# 这是我们的置信度，然后用阈值比较置信度，来分类\n",
    "prob = pd.DataFrame(clf.decision_function(x_valid))\n",
    "prob.loc[prob.iloc[:, 0] >= thresholds[maxindex], \"y_pred\"] = 1\n",
    "prob.loc[prob.iloc[:, 0] < thresholds[maxindex], \"y_pred\"] = 0\n",
    "# 检查缺失值的事情\n",
    "prob.loc[:, \"y_pred\"].isnull().sum()\n",
    "\n",
    "times = time()\n",
    "score = AC(y_valid, prob.loc[:, \"y_pred\"].values)\n",
    "recall = recall_score(y_valid, prob.loc[:, \"y_pred\"])\n",
    "print(\"testing accuracy %f,recall is %f\" % (score, recall))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}