{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 线性回归\n",
    "from sklearn.linear_model import LinearRegression as LR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import fetch_california_housing as fch\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housevalue = fch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housevalue.data # 看不出什么\n",
    "X = pd.DataFrame(housevalue.data) # 放入dataframe中便于查看\n",
    "y = housevalue.target\n",
    "X.shape #(20640, 8)\n",
    "y.shape # (20640,)\n",
    "y.min()\n",
    "y.max() # 不是房价本身，而是房价的评分\n",
    "X.columns = housevalue.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns\n",
    "\"\"\"\n",
    "MedInc: 该街区住户的收入中位数\n",
    "HouseAge: 房屋使用年代的额中位数\n",
    "AveRooms: 平均房屋数量\n",
    "AveBedrms: 平均卧室数量\n",
    "Population: 街区人口\n",
    "AveOccup: 平均入住率\n",
    "Latitude: 街区的纬度\n",
    "Longitude:街区的经度\n",
    "\"\"\"\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=420)\n",
    "x_valid.head()\n",
    "# 恢复索引\n",
    "for i in [x_train, x_valid]:\n",
    "    i.index = range(i.shape[0])\n",
    "x_train.shape\n",
    "# 进行数据标准化，要先分训练集和测试集，再做别的东西，先用训练集训练标准化的类，然后用训练好的类再分别转化训练集和测试集\n",
    "# 建模\n",
    "reg = LR().fit(x_train, y_train)\n",
    "yhat = reg.predict(x_valid)\n",
    "yhat\n",
    "yhat.min()\n",
    "yhat.max() # 比原来的真实值还大了挺多\n",
    "\n",
    "reg.coef_ #w, 系数向量\n",
    "[*zip(x_train.columns, reg.coef_)]\n",
    "# 截距\n",
    "reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 看均方误差\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "MSE(yhat, y_valid) #0.5\n",
    "y_valid.mean() # 2.0, 相比平均值误差已经达到了20%多\n",
    "y.max()\n",
    "y.min()\n",
    "# 用交叉验证进行求解均方误差， 评估指标错了\n",
    "# cross_val_score(reg, X, y, cv=10, scoring=\"mean_squared_error\")\n",
    "# 为什么报错\n",
    "\n",
    "import sklearn\n",
    "sorted(sklearn.metrics.SCORERS.keys()) # 可用的模型评估指标\n",
    "cross_val_score(reg,X, y, cv=10, scoring=\"neg_mean_squared_error\")\n",
    "# 均方误差为负，实际为真，是相反数\n",
    "(cross_val_score(reg, X, y, cv=10, scoring=\"neg_mean_squared_error\") * (-1)).mean()\n",
    "# R^2\n",
    "from sklearn.metrics import r2_score\n",
    "r2_score(yhat, y_valid)\n",
    "r2 = reg.score(x_valid, y_valid)\n",
    "r2\n",
    "# 两种调用的结果R^2是不一样的？\n",
    "r2_score(y_valid, yhat)\n",
    "# 这样就一样了，所以输入的顺序很重要\n",
    "#或者可以指定参数，这样就不会顺序出错了\n",
    "r2_score(y_true=y_valid, y_pred=yhat)\n",
    "cross_val_score(reg,X,y,cv=10,scoring=\"r2\").mean()\n",
    "\n",
    "\n",
    "# 画个图看看\n",
    "import matplotlib.pyplot as plt\n",
    "sorted(y_valid)\n",
    "# 排序是为了不像个散点图\n",
    "plt.figure()\n",
    "plt.plot(range(len(y_valid)), sorted(y_valid), c=\"black\", label=\"Data\")\n",
    "plt.plot(range(len(yhat)), sorted(yhat), c=\"red\", label=\"predict\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 所以R^2才是最重要的指标\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rng = np.random.RandomState(42)\n",
    "X = rng.randn(100, 80)\n",
    "y = rng.randn(100)\n",
    "cross_val_score(LR(), X, y, cv=5, scoring=\"r2\")# 得到5个交叉验证的结果\n",
    "# 出现负数的R^2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 岭回归\n",
    "# 核心参数就是系数a。如果我们的数据集在岭回归中的正则化参数各种取值下没有明显上升，则说明没有多重共线性\n",
    "#，顶多是特征之间存在一些相关性\n",
    "# 反之证明有多重共线性\n",
    "# 重新测试\n",
    "from sklearn.linear_model import Ridge, LinearRegression, Lasso\n",
    "import matplotlib.pyplot as plt\n",
    "housevalue = fch()\n",
    "X = pd.DataFrame(housevalue.data)\n",
    "y = housevalue.target\n",
    "X.columns = [\"住房收入中位数\", \"使用年代中位数\", \"平均房屋数目\", \"平均卧室数目\", \"街区人口\", \"平均入住率\", \"街区的维度\", \"街区的经度\"]\n",
    "X.head()\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=420)\n",
    "for i in [x_train, x_valid]:\n",
    "    i.index = range(i.shape[0])\n",
    "\n",
    "reg = Ridge(alpha=1).fit(x_train, y_train)\n",
    "reg.score(x_valid, y_valid) # 应该不是多重共线性的问题\n",
    "# 下面验证交叉验证下，岭回归的结果如何变动\n",
    "alpharange = np.arange(1, 1001, 100)\n",
    "ridge, lr = [], []\n",
    "for alpha in alpharange:\n",
    "    reg = Ridge(alpha=alpha)\n",
    "    linear = LinearRegression()\n",
    "    regs = cross_val_score(reg, X, y, cv=5, scoring=\"r2\").mean()\n",
    "    linears = cross_val_score(linear, X, y, cv=5, scoring=\"r2\").mean()\n",
    "    ridge.append(regs)\n",
    "    lr.append(linears)\n",
    "plt.figure()\n",
    "plt.plot(alpharange, ridge, color=\"red\", label=\"Ridge\")\n",
    "plt.plot(alpharange, lr, color=\"orange\", label=\"LR\")\n",
    "plt.title(\"Mean\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# 证明只有一些共线性\n",
    "# 再花0 200 之间\n",
    "# 求最优alpha\n",
    "# 使用岭迹图进行选择，已经被淘汰# 直接交叉验证好了\n",
    "# 交叉验证进行岭回归\n",
    "#RidgeCV(\n",
    "    #alphas:需要正则化的参数的元组\n",
    "    # scoring R^2\n",
    "    #store_cv_values是否保存每次结果\n",
    "    # cv :交叉验证模式，默认为cv留一交叉验证\n",
    "#)\n",
    "# cv只有是None才会保存cv的结果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lasso\n",
    "from sklearn.linear_model import Ridge, LinearRegression, Lasso\n",
    "import matplotlib.pyplot as plt\n",
    "housevalue = fch()\n",
    "X = pd.DataFrame(housevalue.data)\n",
    "y = housevalue.target\n",
    "X.columns = [\"住房收入中位数\", \"使用年代中位数\", \"平均房屋数目\", \"平均卧室数目\", \"街区人口\", \"平均入住率\", \"街区的维度\", \"街区的经度\"]\n",
    "X.head()\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=420)\n",
    "for i in [x_train, x_valid]:\n",
    "    i.index = range(i.shape[0])\n",
    "\n",
    "reg = LinearRegression().fit(x_train, y_train)\n",
    "(reg.coef_ * 100).tolist() # coef_ 是系数w\n",
    "Ridge_ = Ridge(alpha=0).fit(x_train,y_train)\n",
    "(Ridge_.coef_ * 100).tolist() # 和线性回归基本一样\n",
    "lasso_ = Lasso(alpha=0).fit(x_train, y_train)\n",
    "(lasso_.coef_ * 100).tolist() # lasso对alpha的变动影响非常大, 而且可以把w压缩到0\n",
    "\n",
    "#lassoCV 因为lasso对正则化系数非常的敏感，所以设定了正则化路径的概念\n",
    "# 正则化路径\n",
    "# 建模应用的是军方误差\n",
    "#alphas_\n",
    "#mse_path 细节结果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "# 自己建立lasso进行alpha选择的范围\n",
    "alpharange = np.logspace(-10, -2, 200,base=10)\n",
    "# 从10 的－10次到10 的－2次取出200个数\n",
    "alpharange.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_ = LassoCV(alphas=alpharange, cv=5).fit(x_train, y_train)\n",
    "lasso_.alpha_\n",
    "lasso_.mse_path_.shape # 返回了每个alpha下的五折交叉验证结果(200, 5)\n",
    "lasso_.mse_path_.mean(axis=1)\n",
    "# 岭回归中我们是axis=0\n",
    "# 岭回归是留一验证，因此我们的交叉验证结果返回的是，每一个样本下再每个alpha下的交叉验证结果\n",
    "# 而在这里，我们返回的是再每一个alpha下每一折的交叉验证的结果\n",
    "#因此我们要求每个alpha下的交叉验证均值，就是axis=1 跨列求均值\n",
    "lasso_.coef_\n",
    "# 最佳正则化洗漱下的系数结果\n",
    "# 贝叶斯信息准则，艾凯克信息准则。来做模型选择，同时还可以利用坐标下降，最小角度回归对lasso进行计算，算是走的最远的模型了\n",
    "# 岭回归和lasso不是为了提升性能而是解决多重共线性而出现的，接下来是多项式回归，专门为了提升性能而设置的算法\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 非线性问题： 多项式回归\n",
    "# 两个的线性关系，在坐标系上是直线，在坐标系上是曲线，是非线性的\n",
    "# 分段的不行，必须 一条 直线 是线性\n",
    "# 线性回归 线性分类 都是最后是个直线\n",
    "# 多元线性回归是线性的，自变量都是一次项\n",
    "# 测试一下各个模型的算法\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "rnd = np.random.RandomState(42)# 设置随机数种子\n",
    "X = rnd.uniform(-3, 3, size=100) # 从设置的两个整数中取出size个随机数,无顺序且无规律\n",
    "# 生成y的思路，先使用numpy中的函数生成一个sin函数图像，然后再人为添加噪声\n",
    "y = np.sin(X) + rnd.normal(size=len(X)) / 3 # normal 生成size个服从正太分布的随机数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用散点图\n",
    "plt.scatter(X, y, marker=\"o\", c=\"k\", s=20)\n",
    "plt.show() # 非线性\n",
    "X.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(-1, 1) # 变成二维数组\n",
    "LinearR =  LinearRegression().fit(X, y)\n",
    "TreeR = DecisionTreeRegressor(random_state=0).fit(X, y)\n",
    "# 放置画布,,画布和绘图要在同一个cell 中\n",
    "fig, ax1 = plt.subplots(1)\n",
    "# 创建测试数据，一系列分布再横坐标上的点\n",
    "line = np.linspace(-3,3, 1000, endpoint=False).reshape(-1, 1)\n",
    "# 将测试数据集带入predict，获得模型的拟合效果并进行绘制\n",
    "ax1.plot(line, LinearR.predict(line), linewidth=2, color=\"green\", label=\"linear regression\")\n",
    "ax1.plot(line, TreeR.predict(line), linewidth=2, color=\"red\", label=\"decdision tree\")\n",
    "# 将原数据上的拟合绘制再图像上\n",
    "ax1.plot(X[:, 0], y, \"o\", c=\"k\")\n",
    "\n",
    "ax1.legend(loc=\"best\")\n",
    "ax1.set_ylabel(\"regression output\")\n",
    "ax1.set_xlabel(\"input feature\")\n",
    "ax1.set_title(\"result before discretization\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 非线性模型在线性上拟合比线性还好，甚至还会过拟合\n",
    "# 线性拟合非线性精度很低, 但是可以用分箱（离散化）来进行处理，甚至比线性的还要好，线性的决策边界都是平行的直线，而非线性的模型的决策边界时曲线或者交叉的直线\n",
    "# 回归问题，自变量上的最高次方1，分类问题，决策边界上最高此方1\n",
    "# knn 朴素贝叶斯等等没有模型这种不区分线性和非线性\n",
    "# 线性回归，逻辑回归，感知机等等为线性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 离散化（分箱)\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "# 将数据分箱\n",
    "enc = KBinsDiscretizer(n_bins=10 # 分几类\n",
    "                        ,encode=\"onehot\"\n",
    "                        )\n",
    "\n",
    "\n",
    "# 做哑变量离散化，形成一个（m， n_bins）每一列是一个分好的类别\n",
    "# 对每一个样本而言，她包含的分类中他表示为1， 其余分类中她表示为0\n",
    "X_binned = enc.fit_transform(X)\n",
    "import pandas as pd\n",
    "pd.DataFrame(X_binned.toarray()).head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行画图\n",
    "enc = KBinsDiscretizer(n_bins=10, encode=\"onehot\")\n",
    "X_binned = enc.fit_transform(X)\n",
    "line_binned = enc.fit_transform(line)\n",
    "# 将两张图绘制在一起，布置画布\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True, figsize=(10, 4))# 让两张图共享y轴的刻度\n",
    "# \n",
    "ax1.plot(line, LinearR.predict(line), linewidth=2, color=\"green\", label=\"linear regression\")\n",
    "ax1.plot(line, TreeR.predict(line), linewidth=2, color=\"red\", label=\"decdision tree\")\n",
    "# 将原数据上的拟合绘制再图像上\n",
    "ax1.plot(X[:, 0], y, \"o\", c=\"k\")\n",
    "\n",
    "ax1.legend(loc=\"best\")\n",
    "ax1.set_ylabel(\"regression output\")\n",
    "ax1.set_xlabel(\"input feature\")\n",
    "ax1.set_title(\"result before discretization\")\n",
    "\n",
    "# 使用分箱数据进行建模\n",
    "LinearR_ = LinearRegression().fit(X_binned, y)\n",
    "TreeR_ = DecisionTreeRegressor(random_state=0).fit(X_binned, y)\n",
    "\n",
    "# 进行预测， 在图二中布置在分箱数据上进行预测的结果\n",
    "ax2.plot(line # 横坐标\n",
    "        , LinearR_.predict(line_binned)\n",
    "        , linewidth=2\n",
    "        , color=\"green\"\n",
    "        , linestyle=\"-\"\n",
    "        , label=\"linear regression\"\n",
    "        )\n",
    "ax2.plot(line, TreeR_.predict(line_binned), linewidth=2, color=\"red\", linestyle=\":\", label=\"decision tree\")\n",
    "# 绘制和箱一致的整线\n",
    "ax2.vlines(enc.bin_edges_[0]#分箱上下限 x轴\n",
    "            , *plt.gca().get_ylim()# y轴的上限和下线 plt.gca取出现有的图像,限制上下限 *是把两个数据分别用的意思\n",
    "            , linewidth=1\n",
    "            , alpha=.2)\n",
    "\n",
    "ax2.plot(X[:, 0], y, \"o\", c=\"k\")\n",
    "ax2.set_ylabel(\"regression output\")\n",
    "ax2.set_xlabel(\"input feature\")\n",
    "ax2.set_title(\"result before discretization\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# 分箱子数量会影响结果\n",
    "\n",
    "#如何选取箱子数呢，交叉验证取值\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score as CVS\n",
    "import numpy as np\n",
    "\n",
    "pred, score, var = [], [], []\n",
    "binsrange = [2, 5, 10, 15, 20, 30]\n",
    "for i in binsrange:\n",
    "    enc = KBinsDiscretizer(n_bins=i, encode=\"onehot\")\n",
    "    X_binned = enc.fit_transform(X)\n",
    "    line_binned = enc.fit_transform(line)\n",
    "    linearR_ = LinearRegression()\n",
    "    # 全数据集上的交叉验证\n",
    "    cvresult = CVS(linearR_, X_binned, y, cv=5)\n",
    "    score.append(cvresult.mean())\n",
    "    var.append(cvresult.var())\n",
    "    # 测试数据集上到额打分结果\n",
    "    pred.append(linearR_.fit(X_binned, y).score(line_binned,np.sin(line)))\n",
    "\n",
    "# 在画图\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多项式回归也可以用线性解决非线性\n",
    "# 高纬呈现，低维解释\n",
    "# 自变量的次数提升，就可以获得数据投影在高位里的结果\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import numpy as np\n",
    "X = np.arange(1, 4).reshape(-1, 1)\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "\n",
    "X_ = poly.fit_transform(X)\n",
    "# [1, 2, 3]\n",
    "# x0 x x^2 x^3\n",
    "# [[1 1 1 1]\n",
    "# [1 2 4 8 ]\n",
    "# [1 3 9 27]]\n",
    "# 这时候在高位中就是线性的了，可以利用线性拟合，然后每个给一个w权重\n",
    "# 次数是n时\n",
    "# [1, X X^2 X^3 X^4 X^n]\n",
    "# 多项式公式\n",
    "# include_bias 是生成x0吗\n",
    "#.coef_ 是权重\n",
    "## 查看截距interccept_\n",
    "# 发现线性回归没有把x0当作截距\n",
    "# 所以可以imclude_bias=False\n",
    "#也可以关闭fit_intercept\n",
    "#LinearRegression(fit_intercept=False).fit(xxx, y).coef_ \n",
    "#LinearRegression(fit_intercept=False).fit(xxx, y).intercept_ # 0\n",
    "\n",
    "\n",
    "# 假如\n",
    "X= np.arange(6).reshape(3, 2)\n",
    "PolynomialFeatures(degree=2).fit_transform(X)\n",
    "# 变成了x0 x1 x2 x1^2 x2^2 x1x2,六个维度\n",
    "# 越高维度交互项越多，约有线性相关性\n",
    "# 所以有参数interaction_only, 布尔值是否只产生交互项\n",
    "# 求解多项式回归 处理非线性数据\n",
    "from sklearn.preprocessing import PolynomialFeatures as PF\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "rnd = np.random.RandomState(42)\n",
    "X = rnd.uniform(-3, 3, size=100)\n",
    "y = np.sin(X) + rnd.normal(size=len(X)) / 3\n",
    "# 将x升维，准备好放入上课Learn中\n",
    "X = X.reshape(-1, 1)\n",
    "# 创建测试数据，\n",
    "line = np.linspace(-3, 3, 1000, endpoint=False).reshape(-1, 1)\n",
    "# 原始特征矩阵的拟合结果\n",
    "LinearR = LinearRegression().fit(X, y)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
    "# 对训练数据进行拟合\n",
    "LinearR.score(X, y)\n",
    "# 对测试数据的拟合\n",
    "LinearR.score(line, np.sin(line))\n",
    "# 多项式拟合，设定高次项\n",
    "d = 5\n",
    "# 进行高次转换\n",
    "poly = PF(degree=d)\n",
    "X_ = poly.fit_transform(X)\n",
    "line_  = PF(degree=d).fit_transform(line)\n",
    "# 训练数据的拟合\n",
    "\n",
    "LinearR_ = LinearRegression().fit(X_, y)\n",
    "LinearR_.score(X_, y)\n",
    "# 测试数据的拟合\n",
    "LinearR_.score(line_, np.sin(line))\n",
    "# 直接起飞\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多项式回归的可解释性\n",
    "from sklearn.linear_model import Ridge, LinearRegression, Lasso\n",
    "import matplotlib.pyplot as plt\n",
    "housevalue = fch()\n",
    "X = pd.DataFrame(housevalue.data)\n",
    "y = housevalue.target\n",
    "X.columns = [\"住房收入中位数\", \"使用年代中位数\", \"平均房屋数目\", \"平均卧室数目\", \"街区人口\", \"平均入住率\", \"街区的维度\", \"街区的经度\"]\n",
    "X.head()\n",
    "y = housevalue.target\n",
    "poly = PolynomialFeatures(degree=2).fit(X, y)\n",
    "# 各种匹配的信息\n",
    "poly.get_feature_names(X.columns)\n",
    "X_ = poly.transform(X)\n",
    "# 我们这时候依然可以直接建立模型，然后使用线性回归的coef——属性来查看什么特征对标签的影响最大\n",
    "reg = LinearRegression().fit(X_, y)\n",
    "coef = reg.coef_\n",
    "[*zip(poly.get_feature_names(X.columns), reg.coef_)]\n",
    "############################放到dataframe中进行排序\n",
    "coeff = pd.DataFrame([poly.get_feature_names(X.columns), reg.coef_.tolist()]).T\n",
    "coeff.columns = [\"feature\", \"coef\"]\n",
    "coeff.sort_values(by=\"coef\")\n",
    "# 可以进行特征创造，比如路程等于速度乘时间\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用随机森林试一试\n",
    "from time import time\n",
    "time0 = time()\n",
    "reg_ = LinearRegression().fit(X_, y)\n",
    "print(\"R2:{}\".format(reg_.score(X_, y)))\n",
    "print(\"time:{}\".format(time()-time0))\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor as RFR\n",
    "time0 = time()\n",
    "print(\"R2:{}\".format(reg_.score(X_, y)))\n",
    "print(\"time:{}\".format(time()-time0))\n",
    "\n",
    "# 多项式回归是线性还是非线性\n",
    "# 自变量合并之后对于y是非线性的，但是就当作直接拿到几个变量， 这样看来就是线性的\n",
    "#狭义认为自变量对于标签不能线性，广义认为参数对于标签不能线性，就是w不能相乘关系,所以定义不同\n",
    "\n",
    "# 所有线性模型都可以多项式化\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ]
}