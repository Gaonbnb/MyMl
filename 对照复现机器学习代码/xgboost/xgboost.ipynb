{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 探索n_estimators对模型的影响\n",
    "from xgboost import XGBRegressor as XGBR\n",
    "from sklearn.ensemble import RandomForestRegressor as RFR\n",
    "from sklearn.linear_model import LinearRegression as LinearR\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import KFold, cross_val_score as CVS, train_test_split as TTS\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time \n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_boston()\n",
    "data.data.shape # 506, 13\n",
    "data.target.shape # 506, \n",
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = TTS(X, y, test_size = 0.3, random_state=420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = XGBR(n_estimators=100).fit(x_train, y_train)\n",
    "reg.predict(x_valid)\n",
    "reg.score(x_valid, y_valid) # 返回的指标是r^2,和其他树的返回是一样的\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE(y_valid, reg.predict(x_valid)) # 均方误差是7.46， 不好不坏\n",
    "reg.feature_importances_ #树模型的优势之一，能够查看模型的重要性分数，可以使用嵌入法(SelectFromModel)进行特征\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = XGBR(n_estimators=100) # 交叉验证中要导入没有经过训练的模型\n",
    "CVS(reg, x_train, y_train, cv=5).mean()\n",
    "# 这里应该返回的是什么指标呢？score返回的就是r^2，和score返回的模型指标是相同的\n",
    "# 交叉验证中，使用的是全数据集还是训练集呢\n",
    "# 严谨或者不严谨 加入放入的是全部的训练集，也是可以的，但是是不严谨的，有把测试数据偷偷告诉模型的嫌疑，所有的数据模型极影见过了\n",
    "# 所以严谨的是先把数据分为训练集和测试集，然后把训练集分为验证集和训练集，再进行交叉验证，之后在test上进行验证好了就是泛化能力好\n",
    "# 但是其实问题不大，加入原本分出来就是不是合适的数据呢，交叉验证本身就是一个不是很严谨的东西，所以没有很大的影响\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CVS(reg, x_train, y_train, cv=5, scoring=\"neg_mean_squared_error\").mean() # -16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看sklearn中的全部评价指标\n",
    "import sklearn\n",
    "sorted(sklearn.metrics.SCORERS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用随机森林和线性回归进行对比\n",
    "rfr = RFR(n_estimators=100)\n",
    "CVS(rfr, x_train, y_train, cv=5).mean() # 0.80\n",
    "CVS(rfr, x_train, y_train, scoring=\"neg_mean_squared_error\").mean() # -18\n",
    "lr = LinearR()\n",
    "CVS(lr, x_train, y_train, cv=5).mean() # 0.68\n",
    "CVS(lr, x_train, y_train, scoring=\"neg_mean_squared_error\").mean() # -26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学习曲线\n",
    "from sklearn.model_selection import learning_curve\n",
    "# 输入我的分类器，一次画出所有的学习曲线\n",
    "def plot_learning_curve(estimator, title, X, y,\n",
    "                        ax, # 选择子图\n",
    "                        ylim=None, #设置纵坐标的取值范围\n",
    "                        cv=None, #交叉验证\n",
    "                        n_jobs=None#设定要素使用的线程\n",
    "                        ):\n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv,n_jobs=n_jobs)\n",
    "    if ax == None:\n",
    "        ax = plt.gca()\n",
    "    else:\n",
    "        ax = plt.figure()\n",
    "    ax.set_title(title)\n",
    "    if ylim is not None:\n",
    "        ax.set_ylim(*ylim) # 保持y轴的量纲相同，使得对比时更加直观\n",
    "    ax.set_xlabel(\"training example\")\n",
    "    ax.set_ylabel(\"score\")\n",
    "    ax.grid() # 显示网格作为背景\n",
    "    ax.plot(train_sizes, np.mean(train_scores, axis=1), \"o-\", color=\"r\", label=\"training score\")\n",
    "    ax.plot(train_sizes, np.mean(test_scores, axis=1), \"o-\", color=\"g\", label= \"test score\")\n",
    "    ax.legend(loc=\"best\")\n",
    "    return ax\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=5, shuffle= True, random_state= 42)\n",
    "# 交叉验证模式，分5份，在分5份之前打乱数据\n",
    "plot_learning_curve(XGBR(n_estimators=100, random_state=420), \"XGB\", x_train, y_train, ax=None,cv=cv)\n",
    "plt.show()\n",
    "# 经常是过拟合的\n",
    "# 训练的好，测试的不好,怎么消除过拟合呢\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 看n_eatimators的学习曲线\n",
    "axisx = range(10,1010,50)\n",
    "rs = []\n",
    "for i in axisx:\n",
    "    reg = XGBR(n_estimators=i, random_state=420)\n",
    "    rs.append(CVS(reg, x_train, y_train, cv=cv).mean())\n",
    "print(axisx[rs.index(max(rs))], max(rs))\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(axisx, rs, C=\"red\", label=\"XGB\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# 30到1000棵基本是差不多的，660棵树判断500多个样本，不太合理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进化的学习曲线 方差与泛化误差\n",
    "axisx = range(50,1010,50)\n",
    "rs = []\n",
    "var = []\n",
    "ge = []\n",
    "for i in axisx:\n",
    "    reg = XGBR(n_estimators=i, random_state=420)\n",
    "    cvresult = CVS(reg, x_train, y_train, cv=cv)\n",
    "    # 记录偏差\n",
    "    rs.append(cvresult.mean())\n",
    "    # 记录方差\n",
    "    var.append(cvresult.var())\n",
    "    # 计算泛化误差的可控部分\n",
    "    ge.append(1 - cvresult.mean() ** 2 + cvresult.var())\n",
    "# 参数的r^2,方差\n",
    "print(axisx[rs.index(max(rs))], max(rs), var[rs.index(max(rs))])\n",
    "\n",
    "print(axisx[rs.index(min(rs))], min(rs), var[rs.index(min(rs))])\n",
    "\n",
    "print(axisx[ge.index(min(ge))], rs[ge.index(min(ge))], var[ge.index(min(ge))], min(ge))\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(axisx, rs, C=\"red\", label=\"XGB\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# 650棵树偏差最低\n",
    "# 50棵树方差最低\n",
    "#150 棵树是泛化误差最小，最好的点\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#添加方差线\n",
    "# 进化的学习曲线 方差与泛化误差\n",
    "axisx = range(100,300,10)\n",
    "rs = []\n",
    "var = []\n",
    "ge = []\n",
    "for i in axisx:\n",
    "    reg = XGBR(n_estimators=i, random_state=420)\n",
    "    cvresult = CVS(reg, x_train, y_train, cv=cv)\n",
    "    # 记录偏差\n",
    "    rs.append(cvresult.mean())\n",
    "    # 记录方差\n",
    "    var.append(cvresult.var())\n",
    "    # 计算泛化误差的可控部分\n",
    "    ge.append(1 - cvresult.mean() ** 2 + cvresult.var())\n",
    "# 参数的r^2,方差\n",
    "print(axisx[rs.index(max(rs))], max(rs), var[rs.index(max(rs))])\n",
    "\n",
    "print(axisx[rs.index(min(rs))], min(rs), var[rs.index(min(rs))])\n",
    "\n",
    "print(axisx[ge.index(min(ge))], rs[ge.index(min(ge))], var[ge.index(min(ge))], min(ge))\n",
    "rs = np.array(rs)\n",
    "var = np.array(var) * .01\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(axisx, rs, C=\"black\", label=\"XGB\")\n",
    "# 添加方差线\n",
    "plt.plot(axisx, rs + var, C=\"red\", linestyle=\"-.\")\n",
    "plt.plot(axisx, rs - var, C=\"red\", linestyle=\"-.\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证模型是否提高\n",
    "time0 = time()\n",
    "print(XGBR(n_estimators=100, random_state=420).fit(x_train, y_train).score(x_valid, y_valid))\n",
    "print(time() - time0)\n",
    "\n",
    "time0 = time()\n",
    "print(XGBR(n_estimators=660, random_state=420).fit(x_train, y_train).score(x_valid, y_valid))\n",
    "print(time() - time0)\n",
    "\n",
    "time0 = time()\n",
    "print(XGBR(n_estimators=180, random_state=420).fit(x_train, y_train).score(x_valid, y_valid))\n",
    "print(time() - time0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 接下来讲解xgboost本身相关\n",
    "for booster in [\"gbtree\", \"gblinear\", \"dart\"]:\n",
    "    reg = XGBR(n_estimators=100, learning_rate=0.1, random_state=420, booster=booster).fit(x_train, y_train)\n",
    "    print(booster)\n",
    "    print(reg.score(x_valid, y_valid))\n",
    "\n",
    "# boston房价是非线性的数据\n",
    "\n",
    "# objective参数\n",
    "# 看一下笔记吧\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb实现法\n",
    "import xgboost as xgb\n",
    "# 使用Dmatrix读取数据\n",
    "dtrain = xgb.DMatrix(x_train, y_train) # 要把特征矩阵和标签都传入才行\n",
    "dvalid = xgb.DMatrix(x_valid, y_valid)\n",
    "import pandas as pd\n",
    "pd.DataFrame(x_train)# 不能打开dtrain，只能提前看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 写明参数\n",
    "# reg:linear is decrea... in favor of squarederror\n",
    "param = {\"silent\": False, \"objective\":\"reg:squarederror\", \"eta\":0.1}\n",
    "num_round = 180\n",
    "bst = xgb.train(param, dtrain, num_round)\n",
    "bst\n",
    "preds = bst.predict(dvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds\n",
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_valid, preds) # 0.92\n",
    "MSE(y_valid,preds) # 6.87 \n",
    "# xgboost底层代码比sklearn要快的多"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gamma参数用sklearn的学习曲线非常的波动，没有什么规律\n",
    "# 我们可以使用xgboost中自带的交叉验证方式\n",
    "import xgboost as xgb\n",
    "# 为了便捷使用全数据\n",
    "dfull = xgb.DMatrix(X, y)\n",
    "param1 = {\"silent\":False, \"obj\":\"reg:linear\", \"gamma\":0}\n",
    "num_round = 180\n",
    "n_fold = 5 # 相当于sklearn中的KFold，就是代表交叉验证\n",
    "#\n",
    "time1 = time()\n",
    "cvresult1 = xgb.cv(param1,dfull,num_round,n_fold)\n",
    "print(datetime.datetime.fromtimestamp(time()-time1).strftime(\"%M:%S:%f\"))\n",
    "cvresult1\n",
    "\n",
    "\n",
    "\n",
    "# 我们定了180轮，就是生成了180*4的交叉验证结果，也就是生成了180棵树，每棵树上都进行了交叉验证\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "plt.grid()\n",
    "plt.plot(range(1, 181),cvresult1.iloc[:, 0],c=\"yellow\",label=\"train gamma=0\")\n",
    "plt.plot(range(1, 181),cvresult1.iloc[:, 2],c=\"red\",label=\"test gamma=0\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#一个先下降再平稳的曲线\n",
    "# xgboost内嵌的评价指标\n",
    "# rmse 回归用，调整后的均方误差\n",
    "# mae 回归用，绝对平均误差\n",
    "#logloss 二分类用，对数损失\n",
    "# mlogloss 多分类用，对数损失\n",
    "#error 分类用，分类误差，等于1-准确率\n",
    "#auc 分类用的auc面积\n",
    "# param1 = {\"silent\":False, \"obj\":\"reg:linear\", \"gamma\":0, \"eval_metric\":\"mae\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始调参\n",
    "dfull = xgb.DMatrix(X, y)\n",
    "param1 = {\"silent\":False, \"obj\":\"reg:linear\", \"gamma\":0}\n",
    "param2 = {\"silent\":False, \"obj\":\"reg:linear\", \"gamma\":20}\n",
    "num_round = 180\n",
    "n_fold = 5 # 相当于sklearn中的KFold，就是代表交叉验证\n",
    "#\n",
    "time1 = time()\n",
    "cvresult1 = xgb.cv(param1,dfull,num_round,n_fold)\n",
    "print(datetime.datetime.fromtimestamp(time()-time1).strftime(\"%M:%S:%f\"))\n",
    "\n",
    "time1 = time()\n",
    "cvresult2 = xgb.cv(param2,dfull,num_round,n_fold)\n",
    "print(datetime.datetime.fromtimestamp(time()-time1).strftime(\"%M:%S:%f\"))\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.grid()\n",
    "plt.plot(range(1, 181),cvresult1.iloc[:, 0],c=\"yellow\",label=\"train gamma=0\")\n",
    "plt.plot(range(1, 181),cvresult1.iloc[:, 2],c=\"red\",label=\"test gamma=0\")\n",
    "plt.plot(range(1, 181),cvresult2.iloc[:, 0],c=\"blue\",label=\"train gamma=20\")\n",
    "plt.plot(range(1, 181),cvresult2.iloc[:, 2],c=\"orange\",label=\"test gamma=20\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# 所以剪枝是后剪枝 ，测试集基本一样，但是训练集gamma=20 不如0\n",
    "# 我们可以看出gamma是通过控制训练集上的训练控制的过拟合，只降低训练集上的表现，所以不一定能够增强测试集上的表现，但是一定能够增强模型的泛化能力\n",
    "# 剪枝不部分都是控制训练集上的学习来防止过拟合的\n",
    "# 分类也是同样的操作\n",
    "# 自行改写吧\n",
    "# 我们倾向于使用这个交叉验证的曲线而不是学习曲线，这个很快\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始剪枝的调参\n",
    "# xgb.train    \n",
    "# xgb.XGBRegressor \n",
    "#max_depth 树的最大深度默认6\n",
    "#coldample_bytree 默认1， 每次生成树随机抽样特征的比例\n",
    "#colsample_bylevel 默认1，每次生成树的一层时，随机抽样特征的比例\n",
    "# colsample_bynode 默认1， N.A 每次生成一个叶子节点时随机抽样特征的比例\n",
    "#min_child_weight 默认1,一个叶子节点上所需要的最后hi，也就是叶子节点上的二阶导数之和，类似于样本权重\n",
    "# 其中max_depth 是剪枝最常用的参数\n",
    "# gamma和max_depth 基本用一个就行\n",
    "# 这里不是只像提升树那样抽样本，而是还可以抽特征，经过证明抽特征比抽取样本效果还好\n",
    "# 我们来试验吧！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfull = xgb.DMatrix(X, y)\n",
    "param1 = {\"silent\":True\n",
    "            ,\"obj\":\"reg:linear\"\n",
    "            ,\"subsample\":1\n",
    "            , \"max_depth\":6\n",
    "            , \"eta\":0.3\n",
    "            , \"gamma\":0\n",
    "            , \"lambda\":1\n",
    "            , \"alpha\":0\n",
    "            , \"colsample_bytree\":1\n",
    "            , \"colsample_bylevel\":1\n",
    "            , \"colsample_bynode\":1\n",
    "            , \"nfold\":5}\n",
    "num_round = 200\n",
    "\n",
    "time0 = time()\n",
    "cvresult1 = xgb.cv(param1, dfull, num_round)\n",
    "print(datetime.datetime.fromtimestamp(time()-time0).strftime(\"%M:%S:%f\"))\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(15,10))\n",
    "\n",
    "ax.set_ylim(top=5)\n",
    "\n",
    "ax.grid()\n",
    "ax.plot(range(1, 201), cvresult1.iloc[:, 0],c=\"red\",label=\"train original\")\n",
    "ax.plot(range(1, 201), cvresult1.iloc[:, 2],c=\"orange\",label=\"test original\")\n",
    "\n",
    "param2 = {\"silent\":True\n",
    "        ,\"obj\":\"reg:linear\"\n",
    "        ,\"nfold\":5}\n",
    "param3 = {\"silent\":True\n",
    "        ,\"obj\":\"reg:linear\"\n",
    "        ,\"nfold\":5}\n",
    "cvresult2 = xgb.cv(param2, dfull, num_round)\n",
    "cvresult3 = xgb.cv(param3, dfull, num_round)\n",
    "ax.plot(range(1, 201), cvresult2.iloc[:, 0],c=\"red\",label=\"train last\")\n",
    "ax.plot(range(1, 201), cvresult2.iloc[:, 2],c=\"orange\",label=\"test last\")\n",
    "ax.plot(range(1, 201), cvresult3.iloc[:, 0],c=\"red\",label=\"train this\")\n",
    "ax.plot(range(1, 201), cvresult3.iloc[:, 2],c=\"orange\",label=\"test this\")\n",
    "ax.legend(fontsize=\"xx-large\") #字体大小\n",
    "plt.show()\n",
    "\n",
    "# 调参就是向着param2里面添加不同的参数,过拟合就减少max_depth,当等于2时，比等于3（向param3中加入）的时候效果好一些\n",
    "# 泛化能力我认为就是测试集的表现，单个参数的话，可能训练集和测试集的差比较重要，改善过拟合后，测试上升，训练下降\n",
    "# 不管怎么调整我们都不希望测试集上的结果下降,最终发现2比较好\n",
    "\n",
    "# 再调整下一个eta参数，param2中是上次有了max_depth=2的，param3加了eta=0.1...通过这个进行修改\n",
    "# 再调整gamma\n",
    "# 是一种贪心的调参算法\n",
    "# 比较推荐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### 保存模型的基本方式\n",
    "# 使用pickle保存和调用模型\n",
    "import pickle\n",
    "dtrain = xgb.DMatrix(x_train, y_train)\n",
    "#设定参数\n",
    "param = {\"silent\":True\n",
    "            ,\"obj\":\"reg:linear\"\n",
    "            ,\"subsample\":1\n",
    "            , \"max_depth\":6\n",
    "            , \"eta\":0.3\n",
    "            , \"gamma\":0\n",
    "            , \"lambda\":1\n",
    "            , \"alpha\":0\n",
    "            , \"colsample_bytree\":1\n",
    "            , \"colsample_bylevel\":1\n",
    "            , \"colsample_bynode\":1\n",
    "            , \"nfold\":5}\n",
    "num_round = 200\n",
    "bst = xgb.train(param, dfull, num_round)\n",
    "\n",
    "# 保存模型\n",
    "pickle.dump(bst,open(\"xgboostonboston.dat\",\"wb\"))\n",
    "# 注意，open中我们往往用w或者r作为读取的方式，但这只能用于文本文件，当我们希望导入的不是文本文件而是模型本身的时候，我们有\n",
    "#wb wr作为读取的模式，其中wb表示二进制写入，wb表示二进制读入\n",
    "\n",
    "# 看看模型被保存在那里\n",
    "import sys\n",
    "sys.path\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split as TTS\n",
    "#from sklearn.metrics import mean_squard_error as MSE\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "data = load_boston()\n",
    "X = data.data\n",
    "y = data.target\n",
    "x_train, x_valid, y_train, y_valid = TTS(X, y, test_size=0.3,random_state=420)\n",
    "\n",
    "\n",
    "# 注意，如果我们保存的模型是xgboost库中建立的模型，则导入的数据类型也必须是xgboost库中的数据类型\n",
    "dtest = xgb.DMatrix(x_valid, y_valid)\n",
    "# 导入模型\n",
    "loaded_model = pickle.load(open(\"xgboostonboston.dat\",\"rb\"))\n",
    "print(\"loaded model from:xgboostonboston.dat\")\n",
    "\n",
    "# 做预测\n",
    "ypreds = loaded_model.predict(dtest)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "MSE(y_valid, ypreds)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用joblib存储\n",
    "bst = xgb.train(param, dtrain,num_round)\n",
    "import joblib\n",
    "joblib.dump(bst,\"xgboost-boston.dat\")\n",
    "loaded_model = joblib.load(\"xgboost-boston.dat\")\n",
    "ypreds = loaded_model.predict(dtest)\n",
    "MSE(y_valid, ypreds)\n",
    "r2_score(y_valid, ypreds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设使用sklearn\n",
    "from xgboost import XGBRegressor as XGBR\n",
    "\n",
    "bst = XGBR(n_estimators=200)\n",
    "joblib.dump(bst, \"xgboost-boston.dat\")\n",
    "loaded_model = joblib.load(\"xgboost-boston.dat\")\n",
    "\n",
    "# 保存的都是训练完的模型,\n",
    "# 这里就可以直接输入numpy结构了\n",
    "loaded_model.fit(x_train,y_train)\n",
    "ypreds = loaded_model.predict(x_valid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgb中的样本不均衡问题，分类问题中\n",
    "#scale_pos_weight 控制负/正的比例\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier as XGBC\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split as TTS\n",
    "from sklearn.metrics import confusion_matrix as cm, recall_score as recall, roc_auc_score as auc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_1 = 500\n",
    "class_2 = 50\n",
    "centers = [[0.0, 0.0], [2.0, 2.0]]\n",
    "clusters_std = [1.5, 0.5]\n",
    "X, y = make_blobs(n_samples=[class_1, class_2],\n",
    "                    centers=centers,\n",
    "                    cluster_std=clusters_std,\n",
    "                    random_state=0, shuffle=False)\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = TTS(X, y, test_size=0.3, random_state=420)\n",
    "(y == 1).sum() / y.shape[0]\n",
    "\n",
    "\n",
    "clf = XGBC().fit(x_train, y_train)\n",
    "# clf = XGBC(scale_pos_weight=10).fit(x_train, y_train)\n",
    "# 学习曲线一下发现20比较好\n",
    "ypred = clf.predict(x_valid)\n",
    "clf.score(x_valid,y_valid) # 默认返回准确率\n",
    "cm(y_valid, ypred,labels=[1,0])\n",
    "recall(y_valid, ypred)\n",
    "auc(y_valid, clf.predict_proba(x_valid)[:, 1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用xgboost格式做一下\n",
    "dtrain = xgb.DMatrix(x_train, y_train)\n",
    "dtest = xgb.DMatrix(x_valid, y_valid)\n",
    "param = {\"object\":\"binary:logistic\", \"scale_pos_weight\":1}\n",
    "num_round = 100\n",
    "\n",
    "bst = xgb.train(param, dtrain, num_round)\n",
    "preds = bst.predict(dtest)\n",
    "\n",
    "preds\n",
    "# 返回的是分类的概率\n",
    "\n",
    "\n",
    "# 自己设定阈值\n",
    "ypred = preds.copy()\n",
    "ypred[ypreds > 0.5] = 1\n",
    "ypred[ypred != 1] = 0\n",
    "\n",
    "# 打印一下从scale_pos_weight 的改变我们的指标怎么改变\n",
    "# for i in zip(names, scale_pos_weight):\n",
    "# 找到最好的调参就是要不调节阈值，要不就调节scale_pos_weight\n",
    "\n",
    "# 其他参数和用法\n",
    "# n_jobs 使用的线程数\n",
    "# base_socre 分类问题中的先验概率，正样本/负样本的比例，回归中一般是0.5\n",
    "# random_state :生成树的随机模式\n",
    "# missing自动处理缺失值\n",
    "# 稀疏矩阵的时候可以直接缺失值当作0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}