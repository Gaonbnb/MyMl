{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 降维\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "iris = load_iris()\n",
    "y = iris.target\n",
    "X = iris.data\n",
    "X.shape\n",
    "# 二维数组，四维矩阵\n",
    "import pandas as pd\n",
    "pd.DataFrame(X)\n",
    "\n",
    "pca = PCA(n_components=2)#实例化,降维2列\n",
    "pca = pca.fit(X)\n",
    "x_dr = pca.transform(X)\n",
    "#x_dr = PCA(2).fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#可视化\n",
    "#y == 0返回true\n",
    "x_dr[y==0, 0]#布尔索引\n",
    "plt.figure()\n",
    "plt.scatter(x_dr[y==0, 0], x_dr[y==0, 1], c=\"red\", label=iris.target_names[0])\n",
    "plt.scatter(x_dr[y==1, 0], x_dr[y==1, 1], c=\"black\", label=iris.target_names[1])\n",
    "plt.scatter(x_dr[y==2, 0], x_dr[y==2, 1], c=\"orange\", label=iris.target_names[2])\n",
    "plt.legend()\n",
    "plt.title('PCA of IRIS dataset')\n",
    "plt.show()\n",
    "# 改良for\n",
    "# 数据本身就非常完美\n",
    "# 返回可解释性方差，降维后每个新特征向量上的信息量大小\n",
    "pca.explained_variance_\n",
    "# 每个新特征向量所占的信息量占原始信息量的百分比，因为有信息的损失\n",
    "pca.explained_variance_ratio_\n",
    "# 大部分信息集中在第一个特征中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 降维后\n",
    "pca_line = PCA().fit(X)\n",
    "pca_line.explained_variance_ratio_\n",
    "import numpy as np\n",
    "#np.cumsum(pac_line.explained_variance_ratio_)#加和加到一\n",
    "pca_line = PCA().fit(X)\n",
    "plt.plot([1, 2, 3, 4],np.cumsum(pac_line.explained_variance_ratio_))\n",
    "plt.xticks([1, 2, 3, 4])\n",
    "plt.xlabel(('number of components variance'))\n",
    "plt.ylabel('cumulative explained variance')\n",
    "plt.show()\n",
    "#选转折点\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_mle = PCA(n_components='mle')\n",
    "pca_mle = pca_mle.fit(X)\n",
    "x_mle = pca_mle.transform(X)\n",
    "x_mle#自动选择了三个特征\n",
    "pca_mle.explained_variance_ratio_.sum()\n",
    "\n",
    "#另一种参数传入方式,知道想要多少信息量\n",
    "pca_f = PCA(n_components=0.97, svd_solver='full')\n",
    "pca_f = pca_f.fit(X)\n",
    "x_f = pca_f.transform(X)\n",
    "pca_f.explained_variance_ratio_\n",
    "#svd奇异值分解器，svdSVD自己有牛批的计算，直接分解出来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############可视化分析V矩阵的作用\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "faces = fetch_lfw_people(min_faces_per_person=60)#实例化,每个人取60人的照片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces.images.shape\n",
    "faces.data.shape\n",
    "#(1277, 2914)行是样本，列是矩阵中所有的特征,1277是矩阵中图像的个数，\n",
    "#(1277, 62, 47)\n",
    "#62是每个图像的特征矩阵的行\n",
    "#47是每个图像的特征矩阵的列\n",
    "# 横竖都是像素，总过有2914个像素\n",
    "# 62 47来可视化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = faces.data\n",
    "#一个画布，用来画子图每个子图是一个对象,后面是不要轴标签\n",
    "fig, axes = plt.subplots(4, 5, figsize=(8, 4), subplot_kw={'xticks':[], 'yticks':[]})\n",
    "#fig是生成一张画布\n",
    "#axes是生成的子图对象，对axes里面的对象逐一处理，就可以画子图了（4 ，5）\n",
    "# axes[0][0]第一个对象，所以是二维结构，两种索引，一个是循环一次生成一列上的图，还有就是拉直了，一次生成一个图\n",
    "#.imshow将图像填充到画布上\n",
    "# 我们要遍历face.images\n",
    "# 要从一个数据集中取出24个图，明显是一次性的循环切片[i,:,:]方便\n",
    "#  所以要拉成一条直线来循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes[0][0].imshow(faces.images[0,:,:])#拿第一张图,并且填到了第一个位置上\n",
    "[*axes.flat]#axes.flat惰性对象，要用【*】打开enumerate(axes.flat)也是惰性对象\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(faces.image[i, :, :],cmap='gray')\n",
    "#cmap 用什么颜色填充图像\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sklearn 只接受2个一下的特征降维\n",
    "pca  = PCA(150).fit(X)#X是face.data因为剩下的两个维度都是特征，取150个特征\n",
    "V = pca.components_# V是v(k,n)矩阵\n",
    "V.shape#(150, 2914)#取150个2914 = 62 * 47\n",
    "\n",
    "###########循环新特征空间的图像\n",
    "fig, axes = plt.subplots(4, 5, figsize=(8, 4), subplot_kw={'xticks':[], 'yticks':[]})\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(V[i,:].reshape(62, 47), cmap='gray')\n",
    "# 得到的是很魔鬼的东西\n",
    "#\n",
    "x_dr = pca.transform(X)\n",
    "x_dr.shape#(1277, 150)这个是降维为150的结果，画出来图像会比较清晰    \n",
    "# 看图，说明和五官关系比较大，所以人脸识别，人的瞳孔是不可能一样的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inverse transform 反向传递例子\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "faces = fetch_lfw_people(min_faces_per_person=60)\n",
    "X = face.data\n",
    "pca = PCA(150)\n",
    "x_dr = \n",
    "x_inverse = pca.inverse_transform(x_dr)\n",
    "x_inverse.shape\n",
    "\n",
    "# 循环了\n",
    "fig, axes = plt.subplots(2, 10, figsize=(10, 2.5), subplot_kw={'xticks':[], 'yticks':[]})\n",
    "#循环将图像填入子图\n",
    "#ax是二行十列，第一行是原数据，第二行是返回的数据\n",
    "#循环一次填两图\n",
    "for i in range(10):\n",
    "    ax[0, i].imshow(face.image[i,:,:],cmap='binary_r')\n",
    "    ax[1, i].imshow(x_inverse[i].reshape(62, 47), cmap='binary_r')\n",
    "#返回的数据模糊了，因为原来有2900个特征，后来的信息是150维升维，之前被舍弃的信息找不回来了，已经不是原来的完全特征了\n",
    "#扫脸可能只提取150个特征，但还是能和身份证上的精确照片匹配，身份证可能有3000维\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用PCA进行噪音过滤\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "digits = load_digits()\n",
    "digits.data.shape\n",
    "digits.images\n",
    "#(1797,64)#64个特征\n",
    "set(digits.target.tolist())\n",
    "def plot_digits(data):\n",
    "    fig, axes = plt.subplots(4, 10,  figsize=(10, 4), subplot_kw={'xticks':[], 'yticks':[]})\n",
    "    for i,ax in enumerate(axes.flat):\n",
    "        ax.imshow(data[i].reshape(8,8), cmap='binary')\n",
    "\n",
    "\n",
    "plot_digits(digits.data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 人为加上噪音\n",
    "import numpy as np\n",
    "np.random.RandomState(42)\n",
    "#在指定的数据集中，随机抽取正态分布数据\n",
    "#两个参数，分别是指定的数据集，和抽取出来的方差\n",
    "noisy = np.random.normal(digits.data, 2)\n",
    "plot_digits(noisy)#带噪音函数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(0.5, svd_solver='full').fit(noisy)#降维后的特征，需要50%的信息\n",
    "x_dr = pca.transform(noisy)\n",
    "x_dr.shape#(1797, 6)\n",
    "without_noise = pca.inverse_transform(x_dr)#一个升为过程，但是噪音删了的东西也没了\n",
    "without_noise.shape\n",
    "#和原图片已经像了\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一个小项目\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/data11686/train.csv')\n",
    "data.shape\n",
    "X = data.iloc[:, 1:]\n",
    "y = data.iloc[:, 0]\n",
    "#画累计方差贡献率\n",
    "pca_line = PCA().fit(X)\n",
    "# 返回784个的方差贡献率\n",
    "plt.figure(figsize=[20, 5])\n",
    "plt.plot(np.cumsum(pca_line.explained_variance_ratio_))\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "plt.show()\n",
    "# 选转折点\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#画降维后的曲线,学习曲线，用随机森林\n",
    "score = []\n",
    "for i in range(1,101, 10):\n",
    "    x_dr = PCA(i).fit_transform(X)\n",
    "    once = cross_val_score(RFC(n_estimators=10,random_state=0), x_dr, y,cv=5).mean()\n",
    "    score.append(once)\n",
    "plt.figure(figsize=[20, 5])\n",
    "plt.plot(range(1, 101, 10),score)\n",
    "plt.show()\n",
    "#用10个特征就已经90%了。。。\n",
    "#用20个比较好\n",
    "# 再画10 25之间的学习曲线，找到更准的\n",
    "# 找到21维就够了。。。随机森林91%准确率，原来300多维96%\n",
    "# 再调整随机森林n_estimators，随机森林94%（max_depth求一下是过拟合还是未拟合）\n",
    "# 还可以换模型knn试试提升精度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "cross_val_score(KNN(),x_dr,y,cv=5)#默认k是5\n",
    "# 精确度高了，能够使用knn因为PCA降维了,所以跑的快了\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#再给knn画个学习曲线\n",
    "score = []\n",
    "x_dr = PCA(21).fit_transform(X)\n",
    "for i in range(10):\n",
    "    once = cross_val_score(KNN(i+1), x_dr, y,cv=5).mean()\n",
    "    score.append(once)\n",
    "plt.figure(figsize=[20, 5])\n",
    "plt.plot(range(1, 101, 10),score)\n",
    "plt.show()"
   ]
  }
 ]
}