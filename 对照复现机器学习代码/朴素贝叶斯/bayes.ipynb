{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits() # 手写数据集\n",
    "X, y = digits.data, digits.target\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=420)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape #(1257, 64)\n",
    "x_valid.shape #(540, 64)\n",
    "np.unique(y_train) # 是个特征\n",
    "# 多分类问题，类别十个\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB().fit(x_train, y_train)\n",
    "acc_score = gnb.score(x_valid, y_valid) # 返回预测的精确性accuracy\n",
    "y_pred = gnb.predict(x_valid)\n",
    "y_pred # 返回所有分类\n",
    "prob = gnb.predict_proba(x_valid)\n",
    "prob # 每一列对应的标签概率\n",
    "# 使用混淆矩阵来查看结果\n",
    "from sklearn.metrics import confusion_matrix as CM\n",
    "CM(y_valid, y_pred) # 模型结果还不错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 高斯朴素贝叶斯,具体的各种形状的分类数据比较可以参考决策树和svm\n",
    "# 月亮型，二分型， 环形...，比较适合线性可分的数据,但是非线性也能做，不是纯粹的线性的，主要问题是需要不同的特征之间需要时独立的\n",
    "# 使用sklearn中自带的绘制学习曲线的类learning_curve，在这个类下执行交叉验证并从中获得不同样本量下的训练和测试准确度\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from time import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GaussianNB()\n",
    "cv = ShuffleSplit(n_splits=50 # 把数据分为多少份\n",
    "                    , test_size=0.2 # 20% * 50份的数据会被作为测试集\n",
    "                    , random_state=0 # 分交叉验证的份数时进行随机抽样的模式\n",
    "                    )\n",
    "train_sizes, train_scores, test_scores = learning_curve(clf, X, y, cv=cv,n_jobs=4)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes #[ 143,  467,  790, 1113, 1437] 每次分训练集和测试集建模之后，训练集上的样本数量\n",
    "train_scores # 训练集上的分数\n",
    "train_scores.shape #(5, 50)\n",
    "test_scores.shape #5个取值之下进行50次交叉验证的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入我的分类器，一次画出所有的学习曲线\n",
    "def plot_learning_curve(estimator, title, X, y,\n",
    "                        ax, # 选择子图\n",
    "                        ylim=None, #设置纵坐标的取值范围\n",
    "                        cv=None, #交叉验证\n",
    "                        n_jobs=None#设定要素使用的线程\n",
    "                        ):\n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv,n_jobs=n_jobs)\n",
    "    ax.set_title(title)\n",
    "    if ylim is not None:\n",
    "        ax.set_ylim(*ylim) # 保持y轴的量纲相同，使得对比时更加直观\n",
    "    ax.set_xlabel(\"training example\")\n",
    "    ax.set_ylabel(\"score\")\n",
    "    ax.grid() # 显示网格作为背景\n",
    "    ax.plot(train_sizes, np.mean(train_scores, axis=1), \"o-\", color=\"r\", label=\"training score\")\n",
    "    ax.plot(train_sizes, np.mean(test_scores, axis=1), \"o-\", color=\"g\", label= \"test score\")\n",
    "    ax.legend(loc=\"best\")\n",
    "    return ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = [\"naive bayes\", \"decisiontree\", \"svm, rbf kernel\", \"ramdomforest\", \"logistic\"]\n",
    "model = [GaussianNB(), DTC(), SVC(gamma=0.001), RFC(n_estimators=50),LR(C=.1, solver=\"lbfgs\")]\n",
    "cv = ShuffleSplit(n_splits=50, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,5,figsize=(30, 6))\n",
    "for ind, title_, estimator in zip(range(len(title)), title, model):\n",
    "    times = time()\n",
    "    plot_learning_curve(estimator, title_, X, y,ax=axes[ind],ylim=[0.7, 1.05],n_jobs=4,cv=cv)\n",
    "    print(\"{}:{}\".format(title_, datetime.datetime.fromtimestamp(time()-times).strftime(\"%M:%S:%f\")))\n",
    "plt.show()\n",
    "# 样本很大的时候，准确率贝叶斯会下降，适合小样本数据,\n",
    "#逻辑回归比贝叶斯效果好，但是维度高会运行非常慢\n",
    "#这两个算法恢复的是一个概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 贝叶斯评估指标\n",
    "# 在二分类问题中，一般会运用布利尔指标，是一种loss\n",
    "from sklearn.metrics import brier_score_loss\n",
    "# 注意第一个参数是真实标签，第二个参数是预测出来的\n",
    "# 二分类下，predict_proba会返回两列，svc的decision_function会返回一列\n",
    "brier_score_loss(y_valid,prob[:,1],pos_label=1)\n",
    "# 我们的pos_label和prob中的索引一致,就可以查看这个类别下的布利尔分数是多少\n",
    "\n",
    "# 多分类中\n",
    "for i in range(0, 10):\n",
    "    print(brier_score_loss(y_valid,prob[:,i],pos_label=i))\n",
    "\n",
    "# 只能返回选择的类的布利尔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 返回概率的模型都可以运用布利尔分数\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "logi = LR(C=.1, solver=\"lbfgs\", max_iter=3000, multi_class=\"auto\").fit(x_train,y_train)\n",
    "svc = SVC(kernel=\"linear\", gamma=1).fit(x_train, y_train)\n",
    "brier_score_loss(y_valid, logi.predict_proba(x_valid)[:, 1],pos_label=1)\n",
    "# 由于svc的置信度并不是概率，为了可比性，我们需要将svc的置信度距离归一化，压缩到0-1之间\n",
    "svc_prob = (svc.decision_function(x_valid) - svc.decision_function(x_valid).min()) / (svc.decision_function(x_valid).max() - svc.decision_function(x_valid).min())\n",
    "brier_score_loss(y_valid, svc_prob[:, 1],pos_label=1)\n",
    "# 支持向量机最差，因为本身就不是返回概率的模型\n",
    "# 接下来进行一下可视化\n",
    "import pandas as pd\n",
    "name = [\"bayes\", \"logistic\", \"svc\"]\n",
    "color = [\"red\", \"black\", \"orange\"]\n",
    "df = pd.DataFrame(index=range(10), columns=name)\n",
    "for i in range(10):\n",
    "    df.loc[i, name[0]] = brier_score_loss(y_valid, prob[:, i], pos_label=i)\n",
    "    df.loc[i, name[1]] = brier_score_loss(y_valid, logi.predict_proba(x_valid)[:, i], pos_label=i)\n",
    "    df.loc[i, name[2]] = brier_score_loss(y_valid, svc_prob[:, i], pos_label=i)\n",
    "for i in range(df.shape[1]):\n",
    "    plt.plot(range(10), df.iloc[:, i],c = color[i])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# 逻辑回归相对最好，支持向量机就很差\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指标对数似然函数log loss\n",
    "# 可以应用于多酚类\n",
    "# y_pred是给的概率\n",
    "from sklearn.metrics import log_loss\n",
    "log_loss(y_valid, prob) #2.4725653911460683\n",
    "log_loss(y_valid, logi.predict_proba(x_valid)) #0.10398581573876683\n",
    "log_loss(y_valid, svc_prob) #1.625556312147472\n",
    "# 在这个评价指标下，支持向量机比贝叶斯好了，因为支持向量机本身就是最优化损失函数，所以会在损失评价下，效果很好\n",
    "# 现实下log_loss进行评价，因为贝叶斯基本用不上\n",
    "#对数似然看多分类，对比多个模型，可解释性差，最优化指向svc，逻辑回归，数学上概率无法取到1或0，只能接近\n",
    "#布利尔看二分类，衡量单一模型，可解释性好，指向朴素贝叶斯，概率可以为0， 1，比如树或者随机森林可以用布利尔\n",
    "# 若贝叶斯算法效果不好，还不换模型，我们要调节校准程度，来强行调整，下面介绍\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可靠性曲线reliability curve\n",
    "#是以预测概率为横坐标，真实标签为纵坐标的曲线，因此模型或者算法的概率校准曲线越靠近对角线越好。通常应用于二分类问题\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from time import time\n",
    "from sklearn.datasets import make_classification as mc\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mc(n_samples=100000, n_features=20 #总共20个特征\n",
    "            , n_classes=2#标签分为两类\n",
    "            ,n_informative=2# 器中两个代表较多信息\n",
    "            ,n_redundant=10 # 是个都是冗余特征\n",
    "            ,random_state=42)\n",
    "# 样本量足够大，因此使用1%的样本作为训练集\n",
    "x_train,x_valid,y_train,y_valid = train_test_split(X, y,test_size=0.99,random_state=42)\n",
    "x_train\n",
    "#######\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(x_train, y_train)\n",
    "y_pred = gnb.predict(x_valid)\n",
    "prob_pos = gnb.predict_proba(x_valid)[:,1]# 我们的预测概率 - 横坐标\n",
    "# 我们的真实标签 - 纵坐标\n",
    "# 利用字典创建dataframe\n",
    "df = pd.DataFrame({\"ytrue\":y_valid[:500],\"probability\":prob_pos[:500]})\n",
    "df\n",
    "df = df.sort_values(\"probability\")\n",
    "# 恢复索引\n",
    "df.index = range(df.shape[0])\n",
    "fig = plt.figure()# 画布\n",
    "ax1 = plt.subplot() # 建立一个子图\n",
    "ax1.plot([0, 1],[0, 1], \"k:\", label=\"perfectly calibrated\")# 做一条对角线来进行对比\n",
    "ax1.plot(df[\"probability\"],df[\"ytrue\"], \"s-\")#, label=\"%s (%1.3f)\" % (\"bayes\", clf_score))\n",
    "ax1.set_ylabel(\"true label\")\n",
    "ax1.set_xlabel(\"predicted probability\")\n",
    "ax1.set_ylim([-0.05, 1.06])\n",
    "ax1.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 这个图就没什么意义了，0 1 之间一直跳，我们应该找真实的概率是多少，但不能求，所以用另一种方式找\n",
    "# 一个简单的做法是，将数据进行分箱，然后规定每个箱子中真实的少数类所占的比例为这个箱子上的真是概率trueproba,这个箱子中预测概率的均值为这个箱子的预测概率\n",
    "#predproba，然后以trueproba为纵坐标，predproba为横坐标，来绘制我们的额可靠性曲线\n",
    "# sklearn中可以通过绘制可靠性曲线的类calibrationcurve实现\n",
    "# 参数normalize对输入归一化，y_pred可以输入正类别下的概率或者置信度\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    " # 从类calibirtion_curve中获取横坐标和纵坐标\n",
    "trueproba, predproba = calibration_curve(y_valid, prob_pos, n_bins=10)\n",
    "\n",
    "fig = plt.figure()# 画布\n",
    "ax1 = plt.subplot() # 建立一个子图\n",
    "ax1.plot([0, 1],[0, 1], \"k:\", label=\"perfectly calibrated\")# 做一条对角线来进行对比\n",
    "ax1.plot(predproba, trueproba, \"s-\")#, label=\"%s (%1.3f)\" % (\"bayes\", clf_score))\n",
    "ax1.set_ylabel(\"true label\")\n",
    "ax1.set_xlabel(\"predicted probability\")\n",
    "ax1.set_ylim([-0.05, 1.06])\n",
    "ax1.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#探索不同箱子数对结果的影响\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
    "for ind, i in enumerate([3, 10, 100]):\n",
    "    ax = axes[ind]\n",
    "    ax.plot([0, 1],[0, 1], \"k:\", label=\"perfectly calibrated\")# 做一条对角线来进行对比\n",
    "    trueproba, predproba = calibration_curve(y_valid, prob_pos, n_bins=i)\n",
    "    ax.plot(predproba, trueproba, \"s-\")#, label=\"%s (%1.3f)\" % (\"bayes\", clf_score))\n",
    "    ax1.set_ylabel(\"true label\")\n",
    "    ax1.set_xlabel(\"predicted probability\")\n",
    "    ax1.set_ylim([-0.05, 1.06])\n",
    "    ax1.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = [\"gaussianbayes\", \"logistic\", \"svc\"]\n",
    "gnb = GaussianNB()\n",
    "logi = LR(C=.1, solver=\"lbfgs\", max_iter=3000, multi_class=\"auto\")\n",
    "svc = SVC(kernel=\"linear\", gamma=1)\n",
    "fig, ax1 = plt.subplots(figsize=(8, 6))\n",
    "ax1.plot([0, 1], [0, 1], \"k:\", label=\"perfectly calibrated\")\n",
    "for clf, name_ in zip([gnb, logi, svc], name):\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_valid)\n",
    "    # hasattr(obj.name): 查看一个类obj中是否存在名字为name的接口，存在则返回true\n",
    "    if hasattr(clf, \"predict_proba\"):\n",
    "        prob_pos = clf.predict_proba(x_valid)[:, 1]\n",
    "    else: # use decision function \n",
    "        prob_pos = clf.decision_function(x_valid)\n",
    "        prob_pos = (prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min())\n",
    "\n",
    "        # 返回布利尔分数\n",
    "    clf_score = brier_score_loss(y_valid, prob_pos, pos_label=y.max())\n",
    "    trueproba, predproba = calibration_curve(y_valid, prob_pos, n_bins=10)\n",
    "    ax1.plot(predproba, trueproba, \"s-\", label=\"%s (%1.3f)\" % (name_, clf_score))\n",
    "\n",
    "ax1.set_ylabel(\"\")\n",
    "ax1.set_xlabel(\"\")\n",
    "ax1.set_ylim([-.05, 1.05])\n",
    "ax1.legend()\n",
    "fig.show()\n",
    "\n",
    "# 逻辑回归天生完美的返回概率的模型\n",
    "\n",
    "# 支持向量机和高斯贝叶斯都是比较糟糕的结果\n",
    "# 对于贝叶斯呈现出来sigmoid函数的镜像的情况，说明数据集中的特征不是相互条件独立的\n",
    "# 支持向量机是典型的置信度不足的分类器\n",
    "# 大量的样本点集中在决策边界的附近， 因此许多样本点的置信度靠近0.5左右，即使决策边界能够将样本点判断正确，模型本身对这个结果也不是非常的新人\n",
    "# 离边界远的点的置信度会很高 ，因为大概率不会被判断错误，所以支持向量机面对混合度较高的数据时，有着天生的置信度不足的缺点\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制直方图来查看预测概率的分布\n",
    "# 这里的分箱是为了把概率分成一个个区间，可靠性曲线中的分箱是为了曲线的平滑\n",
    "fig, ax2 = plt.subplots(figsize=(8, 6))\n",
    "for clf, name_ in zip([gnb, logi, svc], name):\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_valid)\n",
    "    # hasattr(obj.name): 查看一个类obj中是否存在名字为name的接口，存在则返回true\n",
    "    if hasattr(clf, \"predict_proba\"):\n",
    "        prob_pos = clf.predict_proba(x_valid)[:, 1]\n",
    "    else: # use decision function \n",
    "        prob_pos = clf.decision_function(x_valid)\n",
    "        prob_pos = (prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min())\n",
    "    ax2.hist(prob_pos, bins=10, label=name_, histtype=\"step\", lw=2)\n",
    "    # histtype设置直方图为透明，lw是直方图每个柱子的粗细\n",
    "    # 预测概率直接当x，y自己计算属于x区间的取值有多少\n",
    "ax2.set_xticks(list(np.arange(0, 1, 0.1)))\n",
    "ax2.legend()\n",
    "fig.show()\n",
    "\n",
    "# 支持向量机过于自卑， 朴素贝叶斯过于自负， 只有逻辑回归正好\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 校准可靠性曲线 ， 有两种回归的方法， 原理可以自己去查，这些类只有predict_proba接口调用\n",
    "# 参数输入已经训练完的模型\n",
    "#cv默认5折，对于输入整数或者是none，二分类就是自动使用sklearn.model_selection_stratifiedKFold进行分割\n",
    "# y如果是连续性变量，就使用sklearn.medel_selection.KFold进行分割\n",
    "# 或者输入其他类建好的交叉验证模式或者生成器cv\n",
    "# 或者可迭代的， 已经分割好的训练集和测试集索引数组\n",
    "# cv=prefit,则假设已经在分类器上拟合完毕数据， 这种模式下， 使用者要手动确定用来拟合分类器的数据和即将被校准的数据没有交集\n",
    "# method sigmoid 使用基于platt的sigmoid模型来进行校准 或者isotonic使用等渗回归进行校准（倾向过拟合)\n",
    "name = [\"gaussianbayes\", 'logistic', 'bayes_isotonic', 'bayes+signoid']\n",
    "gnb = GaussianNB()\n",
    "models = [gnb, LR(C=.1, solver=\"lbfgs\", max_iter=3000, multi_class=\"auto\"),\n",
    "        CalibretedClassifierCV(gnb, cv=2, method=\"isotonic\"), \n",
    "        CalibretedClassifierCV(gnb, cv=2, method=\"sigmoid\"), \n",
    "        ]\n",
    "# plot_calib是把之前的画图函数给集成了一个函数，然后自动画图\n",
    "plot_calib(models, name, x_train, xvalid, y_train, y_valid)\n",
    "# 结果是等渗修正的比较好， sigmoid修正的稍微差一点\n",
    "# 这时再看一下模型的精确性有没有提升\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "gnb = GaussianNB().fit(x_train, y_train) # 0.86\n",
    "gnb.score(x_valid, y_valid) # 0.11\n",
    "brier_score_loss(y_valid, gnb.predict_proba(x_valid)[:,1], pos_label=1)\n",
    "gnbisotonic = CalibratedClassifierCV(gnb, cv=2, method=\"isotonic\").fit(x_train, y_train)\n",
    "gnbisotonic.score(x_valid, y_valid)#0.86\n",
    "brier_score_loss(y_valid, gnbisotonic.predict_proba(x_valid)[:, 1],pos_label=1) # 0.09\n",
    "# 可靠性曲线更靠谱布利尔下降了，准确率却降低了\n",
    "# 解释一下：不如决策树和支持向量机，概率不是概率而是置信度，而且这些模型的分类也不是靠概率而是靠决策边界，可能存在这类别1下的概率为0.4但样本依旧被分成1的情况，这种情况下模型没信心把这个样本分成1但还是分成1了\n",
    "# 这个时候进行概率校准可能会想着更加错误的方向调整，比如0.4调整更接近0，模型直接判断成0了， 最终判断错误，所以布利尔和精确性相反的趋势\n",
    "# 对于朴素贝叶斯磊说， 我们的贝叶斯时有偏估计，校准后，模型的预测更加接近真实概率，但是一组数据集上可能准确路下降，这取决于我们的测试集有多贴近我们估计的真实样本面貌\n",
    "# 当两者相悖时，以准确率为准，这不是说布利尔指数就无效了，因为概率类模型几乎没有参数调整，我们可以用布利尔指数作为调整，进行概率校准， 指不定就会结果更好\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多项式朴素beiyesi\n",
    "# 比如投硬币就是多项式中的二项分布，掷色子就是多项式分布\n",
    "# 多项式分布擅长分布型变量\n",
    "# 现实生活中，我们使用高斯贝叶斯处理连续性变量\n",
    "# 多项式实验中的实验结果都很具体， 特征往往是次数频率计数出现这样的概念，多是离散的正整数，因此不接受负值的输入\n",
    "#因为以上的特点，所以多项式的特征矩阵大多是稀疏矩阵，经常用于文本分类，使用TF-IDF向量技术，也可以使用简单的单词计数向量配合贝叶斯使用\n",
    "# 多项式的参数alpha:a越大，精确率越低， 布利尔分数越高， 因为平滑选项，会增加噪音\n",
    "# fit_prior时候学习先验概率，不给出就统一先验概率，为相同的概率大小，比如投色子掷硬币先验就是一样的\n",
    "# calss_prior 表示类的先验概率，就是fit_prior为none， 自己提供先验\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造数据集\n",
    "class_1 = 500\n",
    "class_2 = 500\n",
    "centers = [[0.0, 0.0], [2.0, 2.0]] # 设定两个类别的中心\n",
    "clusters_std = [0.5, 0.5] # 设定两个类别的方差\n",
    "X, y = make_blobs(n_samples=[class_1, class_2],\n",
    "                centers=centers,\n",
    "                cluster_std=clusters_std,\n",
    "                random_state=0, shuffle=False)\n",
    "X.shape\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=420)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "mms = MinMaxScaler().fit(x_train)# 在训练集上实例化训练我们的模型,保证没有负值\n",
    "x_train_ = mms.transform(x_train)\n",
    "x_vaild_ = mms.transform(x_vaild)\n",
    "mnb = MultinomialNB().fit(x_train_, y_train)\n",
    "# 重要属性，调用根据数据获取的， 每个标签类的对数先验概率log（P（Y））\n",
    "# 由于概率永远是在01之间，所以先验概率永远是负数\n",
    "mnb.class_log_prior_ # 先验概率非常的相似，所以应该是对半分\n",
    "(y_train == 1).sum() / y_train.shape[0]\n",
    "# 验证确实是对半分\n",
    "mnb.class_log_prior_.shape # 永远等于标签中所带的类别数量\n",
    "# 可以使用np.exp查看真正的概率\n",
    "np.exp(mnb.class_log_prior_)\n",
    "# 重要属性：返回一个固定标签类别下的每个特征的对数概率log(P(Xi|y))\n",
    "mnb.feature_log_prob_ # 每个特征，每个标签\n",
    "# 重要属性：在fit时每个标签类别下包含的样本数\n",
    "# 当fit接口中的sample_weight被设置时,该接口返回的值也会受到加权的影响\n",
    "mnb.class_count_\n",
    "#predict predict_proba 都有\n",
    "# 多项式适合离散的数据，现在时连续的，有几种操作，把x_train转换成分类数据，x_trian没有归一化因为哑变量之后没有负数了\n",
    "from sklearn.preprocessing import KBinsDiscretizer # 对连续性的数据进行分箱\n",
    "kbs = KBinsDiscretizer(n_bins=10, encode=\"onehot\").fit(x_train)\n",
    "x_train_ = kbs.transform(x_train)\n",
    "x_valid_ = kbs.transform(x_valid)\n",
    "x_train_.shape # (700, 20) 2个特征中，每个特征分了10个箱所分出来的哑变量\n",
    "mnb = MultinomialNB().fit(x_train_, y_train)\n",
    "# 分类的结果非常好,所以对分类型的数据效果非常好\n",
    "mnb.score(x_valid_, y_valid)\n",
    "# 伯努利朴素贝叶斯， 服从多元的二项分布\n",
    "#这个类要求将样本转化成二分类特征向量,若不是二分类，就用binarize进行转化\n",
    "# 伯努利更加关注的是是否存在，而多项式贝叶斯更关注出现的次数和频率。因此文本分类中，对于单词出现向量（而不是单词次数向量）训练分类器效果更好\n",
    "# 而且在文档较短的数据集上，效果更好\n",
    "# 参数 alpha binarize设定将特征二值化的阈值，若为none，则二值化默认完成\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "mms = MinMaxScaler().fit(x_train)# 在训练集上实例化训练我们的模型,保证没有负值\n",
    "x_train_ = mms.transform(x_train)\n",
    "x_vaild_ = mms.transform(x_vaild)\n",
    "\n",
    "# 不设置二值化\n",
    "bn1_ = BernoulliNB().fit(x_train_, y_train)\n",
    "#bn1_.score(x_valid_, y_valid)\n",
    "brier_score_loss(y_valid, bn1_.predict_proba(x_valid_)[:, 1], pos_label=1)\n",
    "#设置二值化\n",
    "bn1 = BernoulliNB(binarize=0.5).fit(x_train_, y_train)\n",
    "\n",
    "bn1.score(x_valid_, y_valid)\n",
    "brier_score_loss(y_valid, bn1.predict_log_proba(x_valid_)[:, 1],pos_label=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 样本不均衡问题\n",
    "from sklearn.metrics import brier_score_loss as BS, recall_score, roc_auc_score as AUC\n",
    "class_1 = 50000\n",
    "class_2 = 500\n",
    "centers = [[0.0, 0.0],[5.0, 5.0]]\n",
    "clusters_std = [3, 1]\n",
    "X, y = make_blobs(n_samples=[class_1, class_2], centers=centers, cluster_std=clusters_std,random_state=0, shuffle=False)\n",
    "\n",
    "name = [\"Multinomial\", \"Gaussian\",\"Bernoulli\"]\n",
    "models = [MultinomialNB(), GaussianNB(), BernoulliNB()]\n",
    "# 适合分类型，任意型， 二分类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, clf in zip(name, models):\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=420)\n",
    "    # 数据预处理\n",
    "    if name != \"Gaussian\":\n",
    "        kbs = KBinsDiscretizer(n_bins=10, encode=\"onehot\").fit(x_train)\n",
    "        x_train = kbs.transform(x_train)\n",
    "        x_valid = kbs.transform(x_valid)\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_valid)\n",
    "    proba = clf.predict(x_valid)\n",
    "    score = clf.score(x_valid, y_valid) # 准确率\n",
    "    print(name)\n",
    "    print(\"\\tbrier:{:.3f}\".format(BS(y_valid,proba,pos_label=1)))\n",
    "    print(\"\\taccuracy:{:.3f}\".format(score))\n",
    "    print(\"\\trecall:{:.3f}\".format(recall_score(y_valid, y_pred)))\n",
    "    print(\"\\tauc:{:.3f}\".format(AUC(y_valid, proba)))\n",
    "    # 多项式完全放弃了少数类\n",
    "    # 高斯都分别判断错了一点，但是找少数类还不如投硬币\n",
    "    # 伯努利找少数类还行\n",
    "    # 这时候怎么做才能对少数类进行处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本分析\n",
    "# 首先介绍单词计数向量技术\n",
    "sample = [\"Mashin learning is fascinating, it is wonderful\",\n",
    "            \"Machine learning is a sensational technology\",\n",
    "            \"Elsa is a popular character\"]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer()\n",
    "X = vec.fit_transform(sample)\n",
    "X # 得到了三行十一列的稀疏矩阵，三个样本，11个单词\n",
    "vec.get_feature_names()# 获得每个列的名字\n",
    "#X是特征矩阵，vec是模型训练的结果\n",
    "import pandas as pd\n",
    "# 稀疏矩阵无法输入pandas\n",
    "CVresult = pd.DataFrame(X.toarray(), columns= vec.get_feature_names())\n",
    "CVresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 问题： 第一个是句子本身长会对多项式贝叶斯的分子有较大的权重\n",
    "# 极端情况，一个句子几万个，其他的几个单词，那少单词的句子就完全被忽略了\n",
    "# 有 l2 范式就是消除了上面这个影响\n",
    "# is出现的多，但是没什么语义的影响\n",
    "#is 没有什么意义但是权重还很大，所以改进TF-IDF\n",
    "# 这个算法是一个单词月常见，他的权重就会越小，用这个方式压制频繁出现的无意义的额次\n",
    "from sklearn.feature_extraction.text import   TfidfVectorizer as TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TFIDF()\n",
    "X = vec.fit_transform(sample)\n",
    "TFIDFresult = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "TFIDFresult\n",
    "\n",
    "\n",
    "CVresult.sum(axis=0) # 返回每一列的和，11 个结果\n",
    "\n",
    "CVresult.sum(axis=0) / CVresult.sum(axis=0).sum() # 得到每个特征向量的权重\n",
    "\n",
    "TFIDFresult.sum(axis=0) / TFIDFresult.sum(axis=0).sum()\n",
    "\n",
    "# 将原本出现次数比较多的词，压缩了相对应的权重\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始探索文本数据\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "data = fetch_20newsgroups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.target_names # 不同类型的新闻\n",
    "#里面数据巨大，表示一个类，里面有可调用的参数，我们先查看相应的参数有什么，来帮助我们解题\n",
    "\n",
    "#里面有训练集和测试集\n",
    "# 从categories里面选择想要的新闻类别shuffle表述是否打乱样本顺序\n",
    "# 比如随机梯度下降需要随机性\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "categories = [\"sci.space\",'rec.sport.hockey','talk.politics.guns','talk.politics.mideast'\n",
    "    \n",
    "]\n",
    "train = fetch_20newsgroups(subset=\"train\", categories=categories)\n",
    "test = fetch_20newsgroups(subset=\"test\", categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.target_names\n",
    "len(train.data) # 2303篇文章\n",
    "train.data[0]\n",
    "np.unique(train.target) # 总共有四个类标签\n",
    "# 查看是否有不平衡的问题,没有样本不均衡的问题\n",
    "for i in [1,2,3]:\n",
    "    print(i, (train.target == i).sum()/len(train.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train.data\n",
    "x_valid = train.data\n",
    "y_train = train.target\n",
    "y_valid = train.target\n",
    "tfidf = TFIDF().fit(x_train)\n",
    "x_train_ = tfidf.transform(x_train)\n",
    "x_valid_ = tfidf.transform(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_ # 稀疏矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tosee = pd.DataFrame(x_train_.toarray(), columns=tfidf.get_feature_names())\n",
    "tosee.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在不同的贝叶斯上进行，不适用高斯因为高斯不接受稀疏矩阵\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB, BernoulliNB\n",
    "from sklearn.metrics import brier_score_loss as BS\n",
    "name = [\"Multinomial\",\"Complement\",\"Bernoulli\"]\n",
    "models = [MultinomialNB(),ComplementNB(),BernoulliNB()]\n",
    "for name, clf in zip(name,models):\n",
    "    clf.fit(x_train_, y_train)\n",
    "    y_pred = clf.predict(x_valid_)\n",
    "    proba = clf.predict_proba(x_valid_)\n",
    "    score = clf.score(x_valid_, y_valid)\n",
    "    print(name)\n",
    "    # 4个不同标签取值下的布利尔分数\n",
    "    Bscore = []\n",
    "    for i in range(len(np.unique(y_train))):\n",
    "        bs = BS(y_valid, proba[:, i],pos_label=i)\n",
    "        Bscore.append(bs)\n",
    "        print(\"\\tbrier under{}:{:.3f}\".format(train.target_names[i],bs))\n",
    "    print(\"\\taverage brier:{:.3f}\".format(np.mean(Bscore)))\n",
    "    print(\"\\taccuracy:{:.3f}\".format(score))\n",
    "    print(\"\\n\")\n",
    "# 补集贝叶斯精确度最好，但是不是很稳定，就是罗卜尔指数比较大\n",
    "# 二分类的不好， 多项式的还可以\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 继续看看能不能进行调整进行校准\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "name1 = [\"Multinomial\",\"Multinomial+isotonic\",\"Multinomial+sigmoid\",\"Complement\",\"Complement+isotonic\",\n",
    "\"Complement+sigmoid\",\"Bernoulli\",\"Bernoulli+isotonic\",\"Bernoulli+sigmoid\"]\n",
    "modelss = [MultinomialNB()\n",
    "            ,CalibratedClassifierCV(MultinomialNB(), cv=2, method=\"isotonic\")\n",
    "            ,CalibratedClassifierCV(MultinomialNB(), cv=2, method=\"sigmoid\")\n",
    "            ,ComplementNB()\n",
    "            ,CalibratedClassifierCV(ComplementNB(), cv=2, method=\"isotonic\")\n",
    "            ,CalibratedClassifierCV(ComplementNB(), cv=2, method=\"sigmoid\")\n",
    "            ,BernoulliNB()\n",
    "            ,CalibratedClassifierCV(BernoulliNB(), cv=2, method=\"isotonic\")\n",
    "            ,CalibratedClassifierCV(BernoulliNB(), cv=2, method=\"sigmoid\")]\n",
    "for name, clf in zip(name1,modelss):   \n",
    "    clf.fit(x_train_, y_train)\n",
    "    y_pred = clf.predict(x_valid_)\n",
    "    proba = clf.predict_proba(x_valid_)\n",
    "    score = clf.score(x_valid_, y_valid)\n",
    "    print(name)\n",
    "    # 4个不同标签取值下的布利尔分数\n",
    "    Bscore = []\n",
    "    for i in range(len(np.unique(y_train))):\n",
    "        bs = BS(y_valid, proba[:, i],pos_label=i)\n",
    "        Bscore.append(bs)\n",
    "        print(\"\\tbrier under{}:{:.3f}\".format(train.target_names[i],bs))\n",
    "    print(\"\\taverage brier:{:.3f}\".format(np.mean(Bscore)))\n",
    "    print(\"\\taccuracy:{:.3f}\".format(score))\n",
    "    print(\"\\n\")\n",
    "# 选择精确性最高的模型\n",
    "#compltment+sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}