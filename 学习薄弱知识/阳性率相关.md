# 准确性相关参数的总结
## 基本知识
逻辑在于，你的预测是positive-1和negative-0，true和false描述你本次预测的对错

true positive-TP：预测为1，预测正确即实际1

false positive-FP：预测为1，预测错误即实际0

true negative-TN：预测为0，预测正确即实际0

false negative-FN：预测为0，预测错误即实际1
## 混淆矩阵
直观呈现以上四种情况的样本数

以分类模型中最简单的二分类为例，对于这种问题，我们的模型最终需要判断样本的结果是0还是1，或者说是positive还是negative。

我们通过样本的采集，能够直接知道真实情况下，哪些数据结果是positive，哪些结果是negative。同时，我们通过用样本数据跑出分类型模型的结果，也可以知道模型认为这些数据哪些是positive，哪些是negative。

因此，我们就能得到这样四个基础指标，我称他们是一级指标（最底层的）：

真实值是positive，模型认为是positive的数量（True Positive=TP）
真实值是positive，模型认为是negative的数量（False Negative=FN）：这就是统计学上的第二类错误（Type II Error）
真实值是negative，模型认为是positive的数量（False Positive=FP）：这就是统计学上的第一类错误（Type I Error）
真实值是negative，模型认为是negative的数量（True Negative=TN）
|TP|FP|
|FN|TN|
### 混淆矩阵的指标
希望二四象限的数据有而一三象限的少一些
### 二级指标 
但是，混淆矩阵里面统计的是个数，有时候面对大量的数据，光凭算个数，很难衡量模型的优劣。因此混淆矩阵在基本的统计结果上又延伸了如下4个指标，我称他们是二级指标（通过最底层指标加减乘除得到的）：

准确率（Accuracy）—— 针对整个模型
精确率（Precision）
灵敏度（Sensitivity）：就是召回率（Recall）
特异度（Specificity）
| |公式|意义|
|准确率ACC|accuracy=TP+TN/TP+TN+FP+FN|分类模型所有判断正确的结果占总观测值的比重|
|精确率PPV|precision=TP/TP+FP|在模型预测是positive的所有结果中，模型预测对的比重|
|灵敏度TPR|sensitivity=recall=TP/TP+FN|在真实值是positive的所有结果中，模型预测对的比重|
|特异度TNR|specificity=TN/TN+FP|在真实值是negative的所有结果中，模型预测对的比重|

通过上面的四个二级指标，可以将混淆矩阵中数量的结果转化为0-1之间的比率，便于进行标准化的衡量
拓展后会产生另外一个三级指标
### F1 score
F1 score=2PR/P+R
P代表precision，R代表recall
F1-score指标综合了precision和recall的产出的结果，1代表模型的输出最好，0代表输出的结果最差

## ROC
ROC曲线的横坐标为false positive rate（FPR）：FP/(FP+TN)

假阳性率，即实际无病，但根据筛检被判为有病的百分比。

在实际为0的样本中你预测为1的概率

纵坐标为true positive rate（TPR）：TP/(TP+FN)

真阳性率，即实际有病，但根据筛检被判为有病的百分比。

在实际为1的样本中你预测为1的概率，此处即【召回率】【查全率】recall

接下来我们考虑ROC曲线图中的四个点和一条线。

第一个点，(0,1)，即FPR=0,TPR=1，这意味着无病的没有被误判，有病的都全部检测到，这是一个完美的分类器，它将所有的样本都正确分类。

第二个点，(1,0)，即FPR=1，TPR=0，类似地分析可以发现这是一个最糟糕的分类器，因为它成功避开了所有的正确答案。

第三个点，(0,0)，即FPR=TPR=0，即FP（false positive）=TP（true positive）=0，没病的没有被误判但有病的全都没被检测到，即全部选0

类似的，第四个点（1,1），分类器实际上预测所有的样本都为1。

经过以上的分析可得到：ROC曲线越接近左上角，该分类器的性能越好。

【ROC是如何画出来的】

分类器有概率输出，50%常被作为阈值点，但基于不同的场景，可以通过控制概率输出的阈值来改变预测的标签，这样不同的阈值会得到不同的FPR和TPR。

从0%-100%之间选取任意细度的阈值分别获得FPR和TPR，对应在图中，得到的ROC曲线，阈值的细度控制了曲线的阶梯程度或平滑程度。

一个没有过拟合的二分类器的ROC应该是梯度均匀的

ROC曲线有个很好的特性：当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变。而Precision-Recall曲线会变化剧烈，故ROC经常被使用

## AUC
AUC（Area Under Curve）被定义为ROC曲线下的面积，完全随机的二分类器的AUC为0.5，虽然在不同的阈值下有不同的FPR和TPR，但相对面积更大，更靠近左上角的曲线代表着一个更加稳健的二分类器。
于是Area Under roc Curve(AUC)就出现了。顾名思义，AUC的值就是处于ROC curve下方的那部分面积的大小。通常，AUC的值介于0.5到1.0之间，较大的AUC代表了较好的Performance。
同时针对每一个分类器的ROC曲线，又能找到一个最佳的概率切分点使得自己关注的指标达到最佳水平。

那么这个指标代表什么呢？这个指标想表达的含义，简单来说其实就是随机抽出一对样本（一个正样本，一个负样本），然后用训练得到的分类器来对这两个样本进行预测，预测得到正样本的概率大于负样本概率的概率。
|ID|label|pro|
|A|0|0.1|
|B|0|0.4|
|C|1|0.35|
|D|1|0.8|
假设有4条样本。2个正样本，2个负样本，那么M*N=4。即总共有4个样本对。分别是：

（D,B）,（D,A）,(C,B),（C,A）。

在（D,B）样本对中，正样本D预测的概率大于负样本B预测的概率（也就是D的得分比B高），记为1

同理，对于（C,B）。正样本C预测的概率小于负样本C预测的概率，记为0.

最后可以算得，总共有3个符合正样本得分高于负样本得分，故最后的AUC为1+1+1+0/4